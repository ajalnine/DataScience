{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f58d9dc12fae65dfae0d415a92b9b99b3c78cb94"
   },
   "source": [
    "## Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "6702c34bd58b243538a43f91d8874d641969fe15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15206175070276794567\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4945621811\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6712789416347097058\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:08:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from scipy import stats\n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_yaml, Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Convolution2D, Flatten, \\\n",
    "    MaxPooling2D,Input, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.python.ops import array_ops\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import *\n",
    "from keras.applications.densenet import DenseNet169\n",
    "from keras_applications.resnext import ResNeXt50\n",
    "import albumentations\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "config = tf.ConfigProto(device_count={\"CPU\": 1, \"GPU\" : 1})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_transform = albumentations.Compose([\n",
    "    albumentations.RandomRotate90(p=0.5),\n",
    "    albumentations.Transpose(p=0.5),\n",
    "    albumentations.Flip(p=0.5),\n",
    "    albumentations.OneOf([albumentations.CLAHE(clip_limit=2), \n",
    "                         albumentations.IAASharpen(), \n",
    "                         albumentations.IAAEmboss(), \n",
    "        albumentations.RandomBrightness(), \n",
    "                         albumentations.RandomContrast(),\n",
    "        albumentations.Blur(), \n",
    "                          albumentations.GaussNoise(),\n",
    "                          albumentations.ElasticTransform(),\n",
    "                         ], p=0.25), \n",
    "        albumentations.HueSaturationValue(p=0.25), \n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.10, scale_limit=0.10, rotate_limit=0, p=0.5),\n",
    "        albumentations.Normalize(p=1)\n",
    "    \n",
    "    ])\n",
    "\n",
    "val_transform = albumentations.Compose([\n",
    "    albumentations.Normalize(p=1)\n",
    "    ])\n",
    "\n",
    "test_transform = albumentations.Compose([\n",
    "    albumentations.RandomRotate90(p=0.5),\n",
    "    albumentations.Flip(p=0.5),\n",
    "    albumentations.Normalize(p=1)\n",
    "    ])\n",
    "\n",
    "def preprocess_train(image):\n",
    "    return (train_transform(image = image.astype(np.uint8))['image'])\n",
    "\n",
    "def preprocess_val(image):\n",
    "    return (val_transform(image = image.astype(np.uint8))['image'])\n",
    "    \n",
    "def preprocess_test(image):\n",
    "    return (test_transform(image = image.astype(np.uint8))['image'])\n",
    "\n",
    "def preprocess_np(image):\n",
    "    return (image.astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NPGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size=32, indep=np.empty([0,96,96,3]), dep = np.empty([0]), transform = preprocess_train):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.indep = indep\n",
    "        self.dep = dep\n",
    "        self.index = 0\n",
    "        self.transform = transform\n",
    "        self.tr = (lambda x: transform(image=x))\n",
    "        \n",
    "    def __len__(self):\n",
    "        l = int(np.floor(len(self.indep) / self.batch_size))\n",
    "        if ((len(self.indep) % self.batch_size) >0):\n",
    "            l+=1\n",
    "        return l\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.indep[self.index * self.batch_size:(self.index + 1) * self.batch_size], \\\n",
    "               self.dep[self.index * self.batch_size:(self.index + 1) * self.batch_size]\n",
    "            \n",
    "        X = np.array([self.tr(img.astype(np.uint8)).reshape(96,96,3) for img in X])   \n",
    "        \n",
    "        self.index +=1\n",
    "        if (self.index>=self.indep.shape[0]/self.batch_size):\n",
    "            self.index=0    \n",
    "        return X, y\n",
    "\n",
    "    def reset(self):\n",
    "        self.index=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    " \n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KerasFocalLoss(target, input):\n",
    "    \n",
    "    gamma = 2.\n",
    "    input = tf.cast(input, tf.float32)\n",
    "    \n",
    "    max_val = K.clip(-input, 0, 1)\n",
    "    loss = input - input * target + max_val + K.log(K.exp(-max_val) + K.exp(-input - max_val))\n",
    "    invprobs = tf.log_sigmoid(-input * (target * 2.0 - 1.0))\n",
    "    loss = K.exp(invprobs * gamma) * loss\n",
    "    \n",
    "    return K.mean(K.sum(loss, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getfold(n):\n",
    "    val_folds = [(a + n) % 20 for a in range(0,2)]\n",
    "    train_folds = [(a + n + 2) % 20 for a in range(0,6)]\n",
    "    random.shuffle(train_folds)\n",
    "    \n",
    "    indep = np.empty([0,96,96,3])\n",
    "    dep = np.empty([0])\n",
    "    indep_val = np.empty([0,96,96,3])\n",
    "    dep_val = np.empty([0])\n",
    "    \n",
    "    for i in val_folds:\n",
    "        indep_val = np.append(indep_val, np.load(\"indep_{0}.npy\".format(i)), axis=0)\n",
    "        dep_val = np.append(dep_val, np.load(\"dep_{0}.npy\".format(i)), axis=0)\n",
    "        \n",
    "    for i in train_folds:\n",
    "        indep = np.append(indep, np.load(\"indep_{0}.npy\".format(i)), axis=0)\n",
    "        dep = np.append(dep, np.load(\"dep_{0}.npy\".format(i)), axis=0)\n",
    "    \n",
    "    return indep, dep, indep_val, dep_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    for i in range (0, 20):\n",
    "        K.clear_session()\n",
    "        indep, dep, indep_val, dep_val, res = None, None, None, None, None\n",
    "        indep, dep, indep_val, dep_val = getfold(i)\n",
    "        inputs = Input((96, 96, 3))\n",
    "        base_model = DenseNet169(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n",
    "        x = base_model(inputs)\n",
    "        x.Trainable=False\n",
    "        out = Flatten()(x)\n",
    "        out = Dense(256, activation='relu')(out)\n",
    "        out = Dropout(0.5)(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Dense(64, activation='relu')(out)\n",
    "        out = Dropout(0.5)(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = Dense(1, activation='sigmoid')(out)\n",
    "        \n",
    "        gs1 = Model(inputs, out)\n",
    "        gs1.compile(Adam(lr=0.001), loss=\"binary_crossentropy\", metrics=['accuracy']) \n",
    "\n",
    "        with open(r\"med_dn169_{0}.yaml\".format(i), \"w\") as yaml_file:\n",
    "            yaml_file.write(gs1.to_yaml())\n",
    "        with open(r\"med_dn169_{0}.yaml\".format(i+20), \"w\") as yaml_file:\n",
    "            yaml_file.write(gs1.to_yaml())\n",
    "        with open(r\"med_dn169_{0}.yaml\".format(i+40), \"w\") as yaml_file:\n",
    "            yaml_file.write(gs1.to_yaml())\n",
    "        clr_triangular = CyclicLR(base_lr=5e-6, max_lr=0.005, mode=\"triangular2\", step_size=1000)\n",
    "        gs1.fit_generator(generator=NPGenerator(indep=indep, dep=dep, batch_size=64, transform=preprocess_train), \n",
    "                          validation_data=NPGenerator(indep=indep_val, dep=dep_val, batch_size=64, transform=preprocess_val), \n",
    "                          steps_per_epoch=indep.shape[0]/2/64,\n",
    "                          validation_steps=indep_val.shape[0]/64,\n",
    "                          epochs=100, verbose=1, callbacks = \n",
    "                          [\n",
    "                          ModelCheckpoint(\"med_dn169_{0}.h5\".format(i), monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n",
    "                          ModelCheckpoint(\"med_dn169_{0}.h5\".format(i+40), monitor='acc', verbose=1, save_best_only=True, mode='max'),\n",
    "                          ])\n",
    "        gs1.save_weights(\"med_dn169_{0}.h5\".format(i+20))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 0\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "40 1\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "40 2\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "40 3\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "40 4\n",
      "90/90 [==============================] - 81s 895ms/step\n",
      "40 5\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "40 6\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "40 7\n",
      "90/90 [==============================] - 80s 888ms/step\n",
      "40 8\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "40 9\n",
      "90/90 [==============================] - 79s 876ms/step\n",
      "40 10\n",
      "90/90 [==============================] - 79s 874ms/step\n",
      "40 11\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "40 12\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "40 13\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "40 14\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "40 15\n",
      "90/90 [==============================] - 80s 886ms/step\n",
      "41 0\n",
      "90/90 [==============================] - 84s 939ms/step\n",
      "41 1\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "41 2\n",
      "90/90 [==============================] - 80s 888ms/step\n",
      "41 3\n",
      "90/90 [==============================] - 80s 884ms/step\n",
      "41 4\n",
      "90/90 [==============================] - 79s 881ms/step\n",
      "41 5\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "41 6\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "41 7\n",
      "90/90 [==============================] - 79s 881ms/step\n",
      "41 8\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "41 9\n",
      "90/90 [==============================] - 79s 878ms/step\n",
      "41 10\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "41 11\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "41 12\n",
      "90/90 [==============================] - 80s 884ms/step\n",
      "41 13\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "41 14\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "41 15\n",
      "90/90 [==============================] - 79s 879ms/step\n",
      "42 0\n",
      "90/90 [==============================] - 83s 919ms/step\n",
      "42 1\n",
      "90/90 [==============================] - 81s 900ms/step\n",
      "42 2\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "42 3\n",
      "90/90 [==============================] - 82s 906ms/step\n",
      "42 4\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "42 5\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "42 6\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "42 7\n",
      "90/90 [==============================] - 81s 901ms/step\n",
      "42 8\n",
      "90/90 [==============================] - 81s 901ms/step\n",
      "42 9\n",
      "90/90 [==============================] - 82s 915ms/step\n",
      "42 10\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "42 11\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "42 12\n",
      "90/90 [==============================] - 83s 926ms/step\n",
      "42 13\n",
      "90/90 [==============================] - 83s 924ms/step\n",
      "42 14\n",
      "90/90 [==============================] - 82s 915ms/step\n",
      "42 15\n",
      "90/90 [==============================] - 82s 909ms/step\n",
      "43 0\n",
      "90/90 [==============================] - 86s 951ms/step\n",
      "43 1\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "43 2\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "43 3\n",
      "90/90 [==============================] - 81s 903ms/step\n",
      "43 4\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "43 5\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "43 6\n",
      "90/90 [==============================] - 81s 900ms/step\n",
      "43 7\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "43 8\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "43 9\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "43 10\n",
      "90/90 [==============================] - 82s 909ms/step\n",
      "43 11\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "43 12\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "43 13\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "43 14\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "43 15\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "44 0\n",
      "90/90 [==============================] - 88s 980ms/step\n",
      "44 1\n",
      "90/90 [==============================] - 84s 932ms/step\n",
      "44 2\n",
      "90/90 [==============================] - 83s 925ms/step\n",
      "44 3\n",
      "90/90 [==============================] - 84s 929ms/step\n",
      "44 4\n",
      "90/90 [==============================] - 83s 922ms/step\n",
      "44 5\n",
      "90/90 [==============================] - 83s 920ms/step\n",
      "44 6\n",
      "90/90 [==============================] - 83s 924ms/step\n",
      "44 7\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "44 8\n",
      "90/90 [==============================] - 82s 910ms/step\n",
      "44 9\n",
      "90/90 [==============================] - 82s 913ms/step\n",
      "44 10\n",
      "90/90 [==============================] - 82s 917ms/step\n",
      "44 11\n",
      "90/90 [==============================] - 81s 902ms/step\n",
      "44 12\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "44 13\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "44 14\n",
      "90/90 [==============================] - 84s 930ms/step\n",
      "44 15\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "45 0\n",
      "90/90 [==============================] - 86s 961ms/step\n",
      "45 1\n",
      "90/90 [==============================] - 82s 906ms/step\n",
      "45 2\n",
      "90/90 [==============================] - 83s 920ms/step\n",
      "45 3\n",
      "90/90 [==============================] - 82s 907ms/step\n",
      "45 4\n",
      "90/90 [==============================] - 83s 925ms/step\n",
      "45 5\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "45 6\n",
      "90/90 [==============================] - 82s 909ms/step\n",
      "45 7\n",
      "90/90 [==============================] - 81s 903ms/step\n",
      "45 8\n",
      "90/90 [==============================] - 82s 907ms/step\n",
      "45 9\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "45 10\n",
      "90/90 [==============================] - 82s 910ms/step\n",
      "45 11\n",
      "90/90 [==============================] - 82s 907ms/step\n",
      "45 12\n",
      "90/90 [==============================] - 83s 924ms/step\n",
      "45 13\n",
      "90/90 [==============================] - 82s 909ms/step\n",
      "45 14\n",
      "90/90 [==============================] - 82s 911ms/step\n",
      "45 15\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "46 0\n",
      "90/90 [==============================] - 86s 950ms/step\n",
      "46 1\n",
      "90/90 [==============================] - 81s 897ms/step\n",
      "46 2\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "46 3\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "46 4\n",
      "90/90 [==============================] - 82s 912ms/step\n",
      "46 5\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "46 6\n",
      "90/90 [==============================] - 79s 880ms/step\n",
      "46 7\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "46 8\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "46 9\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "46 10\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "46 11\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "46 12\n",
      "90/90 [==============================] - 79s 879ms/step\n",
      "46 13\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "46 14\n",
      "90/90 [==============================] - 79s 881ms/step\n",
      "46 15\n",
      "90/90 [==============================] - 81s 900ms/step\n",
      "47 0\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "47 1\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "47 2\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "47 3\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "47 4\n",
      "90/90 [==============================] - 81s 895ms/step\n",
      "47 5\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "47 6\n",
      "90/90 [==============================] - 80s 887ms/step\n",
      "47 7\n",
      "90/90 [==============================] - 81s 897ms/step\n",
      "47 8\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "47 9\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "47 10\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "47 11\n",
      "90/90 [==============================] - 80s 886ms/step\n",
      "47 12\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "47 13\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "47 14\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "47 15\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "48 0\n",
      "90/90 [==============================] - 93s 1s/step\n",
      "48 1\n",
      "90/90 [==============================] - 87s 971ms/step\n",
      "48 2\n",
      "90/90 [==============================] - 88s 978ms/step\n",
      "48 3\n",
      "90/90 [==============================] - 88s 975ms/step\n",
      "48 4\n",
      "90/90 [==============================] - 89s 984ms/step\n",
      "48 5\n",
      "90/90 [==============================] - 87s 966ms/step\n",
      "48 6\n",
      "90/90 [==============================] - 87s 970ms/step\n",
      "48 7\n",
      "90/90 [==============================] - 87s 969ms/step\n",
      "48 8\n",
      "90/90 [==============================] - 88s 972ms/step\n",
      "48 9\n",
      "90/90 [==============================] - 87s 964ms/step\n",
      "48 10\n",
      "90/90 [==============================] - 87s 972ms/step\n",
      "48 11\n",
      "90/90 [==============================] - 87s 967ms/step\n",
      "48 12\n",
      "90/90 [==============================] - 88s 982ms/step\n",
      "48 13\n",
      "90/90 [==============================] - 88s 982ms/step\n",
      "48 14\n",
      "90/90 [==============================] - 89s 989ms/step\n",
      "48 15\n",
      "90/90 [==============================] - 88s 979ms/step\n",
      "49 0\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "49 1\n",
      "90/90 [==============================] - 82s 907ms/step\n",
      "49 2\n",
      "90/90 [==============================] - 82s 909ms/step\n",
      "49 3\n",
      "90/90 [==============================] - 82s 910ms/step\n",
      "49 4\n",
      "90/90 [==============================] - 82s 911ms/step\n",
      "49 5\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "49 6\n",
      "90/90 [==============================] - 82s 909ms/step\n",
      "49 7\n",
      "90/90 [==============================] - 81s 905ms/step\n",
      "49 8\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "49 9\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "49 10\n",
      "90/90 [==============================] - 81s 895ms/step\n",
      "49 11\n",
      "90/90 [==============================] - 81s 895ms/step\n",
      "49 12\n",
      "90/90 [==============================] - 81s 897ms/step\n",
      "49 13\n",
      "90/90 [==============================] - 81s 903ms/step\n",
      "49 14\n",
      "90/90 [==============================] - 81s 903ms/step\n",
      "49 15\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "50 0\n",
      "90/90 [==============================] - 93s 1s/step\n",
      "50 1\n",
      "90/90 [==============================] - 88s 975ms/step\n",
      "50 2\n",
      "90/90 [==============================] - 87s 968ms/step\n",
      "50 3\n",
      "90/90 [==============================] - 88s 978ms/step\n",
      "50 4\n",
      "90/90 [==============================] - 88s 981ms/step\n",
      "50 5\n",
      "90/90 [==============================] - 89s 992ms/step\n",
      "50 6\n",
      "90/90 [==============================] - 87s 969ms/step\n",
      "50 7\n",
      "90/90 [==============================] - 87s 967ms/step\n",
      "50 8\n",
      "90/90 [==============================] - 87s 967ms/step\n",
      "50 9\n",
      "90/90 [==============================] - 88s 983ms/step\n",
      "50 10\n",
      "90/90 [==============================] - 86s 960ms/step\n",
      "50 11\n",
      "90/90 [==============================] - 86s 957ms/step\n",
      "50 12\n",
      "90/90 [==============================] - 87s 965ms/step\n",
      "50 13\n",
      "90/90 [==============================] - 86s 958ms/step\n",
      "50 14\n",
      "90/90 [==============================] - 85s 947ms/step\n",
      "50 15\n",
      "90/90 [==============================] - 87s 962ms/step\n",
      "51 0\n",
      "90/90 [==============================] - 93s 1s/step\n",
      "51 1\n",
      "90/90 [==============================] - 88s 976ms/step\n",
      "51 2\n",
      "90/90 [==============================] - 90s 1s/step\n",
      "51 3\n",
      "90/90 [==============================] - 89s 994ms/step\n",
      "51 4\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "51 5\n",
      "90/90 [==============================] - 89s 992ms/step\n",
      "51 6\n",
      "90/90 [==============================] - 90s 998ms/step\n",
      "51 7\n",
      "90/90 [==============================] - 89s 989ms/step\n",
      "51 8\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "51 9\n",
      "90/90 [==============================] - 90s 996ms/step\n",
      "51 10\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "51 11\n",
      "90/90 [==============================] - 89s 991ms/step\n",
      "51 12\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "51 13\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "51 14\n",
      "90/90 [==============================] - 90s 1s/step\n",
      "51 15\n",
      "90/90 [==============================] - 90s 1s/step\n",
      "52 0\n",
      "90/90 [==============================] - 79s 879ms/step\n",
      "52 1\n",
      "90/90 [==============================] - 79s 875ms/step\n",
      "52 2\n",
      "90/90 [==============================] - 79s 878ms/step\n",
      "52 3\n",
      "90/90 [==============================] - 80s 886ms/step\n",
      "52 4\n",
      "90/90 [==============================] - 79s 881ms/step\n",
      "52 5\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "52 6\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "52 7\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "52 8\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "52 9\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "52 10\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "52 11\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "52 12\n",
      "90/90 [==============================] - 80s 886ms/step\n",
      "52 13\n",
      "90/90 [==============================] - 79s 880ms/step\n",
      "52 14\n",
      "90/90 [==============================] - 79s 882ms/step\n",
      "52 15\n",
      "90/90 [==============================] - 80s 884ms/step\n",
      "53 0\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "53 1\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "53 2\n",
      "90/90 [==============================] - 80s 891ms/step\n",
      "53 3\n",
      "90/90 [==============================] - 81s 901ms/step\n",
      "53 4\n",
      "90/90 [==============================] - 81s 897ms/step\n",
      "53 5\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "53 6\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "53 7\n",
      "90/90 [==============================] - 81s 902ms/step\n",
      "53 8\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "53 9\n",
      "90/90 [==============================] - 81s 895ms/step\n",
      "53 10\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "53 11\n",
      "90/90 [==============================] - 81s 902ms/step\n",
      "53 12\n",
      "90/90 [==============================] - 81s 902ms/step\n",
      "53 13\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "53 14\n",
      "90/90 [==============================] - 81s 897ms/step\n",
      "53 15\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "54 0\n",
      "90/90 [==============================] - 97s 1s/step\n",
      "54 1\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "54 2\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "54 3\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "54 4\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "54 5\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "54 6\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "54 7\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "54 8\n",
      "90/90 [==============================] - 93s 1s/step\n",
      "54 9\n",
      "90/90 [==============================] - 93s 1s/step\n",
      "54 10\n",
      "90/90 [==============================] - 92s 1s/step\n",
      "54 11\n",
      "90/90 [==============================] - 90s 997ms/step\n",
      "54 12\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "54 13\n",
      "90/90 [==============================] - 89s 994ms/step\n",
      "54 14\n",
      "90/90 [==============================] - 93s 1s/step\n",
      "54 15\n",
      "90/90 [==============================] - 91s 1s/step\n",
      "55 0\n",
      "90/90 [==============================] - 83s 925ms/step\n",
      "55 1\n",
      "90/90 [==============================] - 79s 877ms/step\n",
      "55 2\n",
      "90/90 [==============================] - 79s 881ms/step\n",
      "55 3\n",
      "90/90 [==============================] - 79s 877ms/step\n",
      "55 4\n",
      "90/90 [==============================] - 79s 881ms/step\n",
      "55 5\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "55 6\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "55 7\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "55 8\n",
      "90/90 [==============================] - 82s 906ms/step\n",
      "55 9\n",
      "90/90 [==============================] - 81s 898ms/step\n",
      "55 10\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "55 11\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "55 12\n",
      "90/90 [==============================] - 80s 891ms/step\n",
      "55 13\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "55 14\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "55 15\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "56 0\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "56 1\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "56 2\n",
      "90/90 [==============================] - 80s 888ms/step\n",
      "56 3\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "56 4\n",
      "90/90 [==============================] - 80s 889ms/step\n",
      "56 5\n",
      "90/90 [==============================] - 80s 888ms/step\n",
      "56 6\n",
      "90/90 [==============================] - 80s 892ms/step\n",
      "56 7\n",
      "90/90 [==============================] - 80s 888ms/step\n",
      "56 8\n",
      "90/90 [==============================] - 80s 893ms/step\n",
      "56 9\n",
      "90/90 [==============================] - 80s 890ms/step\n",
      "56 10\n",
      "90/90 [==============================] - 80s 887ms/step\n",
      "56 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 80s 892ms/step\n",
      "56 12\n",
      "90/90 [==============================] - 80s 885ms/step\n",
      "56 13\n",
      "90/90 [==============================] - 81s 896ms/step\n",
      "56 14\n",
      "90/90 [==============================] - 80s 887ms/step\n",
      "56 15\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "57 0\n",
      "90/90 [==============================] - 87s 964ms/step\n",
      "57 1\n",
      "90/90 [==============================] - 82s 916ms/step\n",
      "57 2\n",
      "90/90 [==============================] - 83s 920ms/step\n",
      "57 3\n",
      "90/90 [==============================] - 83s 926ms/step\n",
      "57 4\n",
      "90/90 [==============================] - 83s 918ms/step\n",
      "57 5\n",
      "90/90 [==============================] - 83s 919ms/step\n",
      "57 6\n",
      "90/90 [==============================] - 82s 910ms/step\n",
      "57 7\n",
      "90/90 [==============================] - 83s 921ms/step\n",
      "57 8\n",
      "90/90 [==============================] - 83s 917ms/step\n",
      "57 9\n",
      "90/90 [==============================] - 83s 924ms/step\n",
      "57 10\n",
      "90/90 [==============================] - 83s 921ms/step\n",
      "57 11\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "57 12\n",
      "90/90 [==============================] - 83s 925ms/step\n",
      "57 13\n",
      "90/90 [==============================] - 84s 931ms/step\n",
      "57 14\n",
      "90/90 [==============================] - 84s 935ms/step\n",
      "57 15\n",
      "90/90 [==============================] - 84s 932ms/step\n",
      "58 0\n",
      "90/90 [==============================] - 81s 901ms/step\n",
      "58 1\n",
      "90/90 [==============================] - 81s 902ms/step\n",
      "58 2\n",
      "90/90 [==============================] - 81s 902ms/step\n",
      "58 3\n",
      "90/90 [==============================] - 81s 899ms/step\n",
      "58 4\n",
      "90/90 [==============================] - 82s 907ms/step\n",
      "58 5\n",
      "90/90 [==============================] - 81s 904ms/step\n",
      "58 6\n",
      "90/90 [==============================] - 82s 906ms/step\n",
      "58 7\n",
      "90/90 [==============================] - 82s 906ms/step\n",
      "58 8\n",
      "90/90 [==============================] - 82s 907ms/step\n",
      "58 9\n",
      "90/90 [==============================] - 83s 923ms/step\n",
      "58 10\n",
      "90/90 [==============================] - 85s 946ms/step\n",
      "58 11\n",
      "90/90 [==============================] - 84s 934ms/step\n",
      "58 12\n",
      "90/90 [==============================] - 85s 949ms/step\n",
      "58 13\n",
      "90/90 [==============================] - 85s 943ms/step\n",
      "58 14\n",
      "90/90 [==============================] - 85s 949ms/step\n",
      "58 15\n",
      "90/90 [==============================] - 84s 932ms/step\n",
      "59 0\n",
      "90/90 [==============================] - 83s 923ms/step\n",
      "59 1\n",
      "90/90 [==============================] - 83s 920ms/step\n",
      "59 2\n",
      "90/90 [==============================] - 83s 917ms/step\n",
      "59 3\n",
      "90/90 [==============================] - 82s 908ms/step\n",
      "59 4\n",
      "90/90 [==============================] - 82s 915ms/step\n",
      "59 5\n",
      "90/90 [==============================] - 84s 930ms/step\n",
      "59 6\n",
      "90/90 [==============================] - 83s 918ms/step\n",
      "59 7\n",
      "90/90 [==============================] - 83s 917ms/step\n",
      "59 8\n",
      "90/90 [==============================] - 83s 927ms/step\n",
      "59 9\n",
      "90/90 [==============================] - 84s 928ms/step\n",
      "59 10\n",
      "90/90 [==============================] - 83s 926ms/step\n",
      "59 11\n",
      "90/90 [==============================] - 83s 922ms/step\n",
      "59 12\n",
      "90/90 [==============================] - 83s 919ms/step\n",
      "59 13\n",
      "90/90 [==============================] - 83s 927ms/step\n",
      "59 14\n",
      "90/90 [==============================] - 83s 922ms/step\n",
      "59 15\n",
      "90/90 [==============================] - 82s 915ms/step\n"
     ]
    }
   ],
   "source": [
    "res = np.load(\"res.npy\")\n",
    "data = pd.DataFrame()\n",
    "model =[]\n",
    "for i in range(40, 60):\n",
    "    with open(r\"med_dn169_{0}.yaml\".format(i), \"r\") as yaml_file:\n",
    "        K.clear_session()\n",
    "        m = model_from_yaml(yaml_file.read())\n",
    "        m.load_weights(r\"med_dn169_{0}.h5\".format(i))\n",
    "        for j in range(0,16):\n",
    "            print (i, j)\n",
    "            tg = NPGenerator(indep=res, batch_size=640, transform=preprocess_test)\n",
    "            a = m.predict_generator(tg ,verbose=1, steps=len(tg))\n",
    "            data[\"model_{0}\".format(i * 16 + j)] = pd.Series(a.reshape(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "395/394 [==============================] - 214s 542ms/step - loss: 0.2317 - acc: 0.9233 - val_loss: 0.5901 - val_acc: 0.8181\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.92330, saving model to med_dn169_l0.h5\n",
      "Epoch 2/10\n",
      "395/394 [==============================] - 179s 453ms/step - loss: 0.0850 - acc: 0.9709 - val_loss: 0.5362 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00002: acc improved from 0.92330 to 0.97087, saving model to med_dn169_l0.h5\n",
      "Epoch 3/10\n",
      "395/394 [==============================] - 177s 447ms/step - loss: 0.1846 - acc: 0.9354 - val_loss: 0.5785 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.97087\n",
      "Epoch 4/10\n",
      "395/394 [==============================] - 180s 455ms/step - loss: 0.0749 - acc: 0.9750 - val_loss: 0.5003 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00004: acc improved from 0.97087 to 0.97502, saving model to med_dn169_l0.h5\n",
      "Epoch 5/10\n",
      "395/394 [==============================] - 174s 439ms/step - loss: 0.1548 - acc: 0.9447 - val_loss: 0.5856 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.97502\n",
      "Epoch 6/10\n",
      "395/394 [==============================] - 174s 440ms/step - loss: 0.0740 - acc: 0.9753 - val_loss: 0.4859 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00006: acc improved from 0.97502 to 0.97526, saving model to med_dn169_l0.h5\n",
      "Epoch 7/10\n",
      "395/394 [==============================] - 175s 443ms/step - loss: 0.1446 - acc: 0.9475 - val_loss: 0.5670 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.97526\n",
      "Epoch 8/10\n",
      "395/394 [==============================] - 176s 446ms/step - loss: 0.0736 - acc: 0.9741 - val_loss: 0.4839 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.97526\n",
      "Epoch 9/10\n",
      "395/394 [==============================] - 174s 440ms/step - loss: 0.1317 - acc: 0.9509 - val_loss: 0.5693 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.97526\n",
      "Epoch 10/10\n",
      "395/394 [==============================] - 175s 442ms/step - loss: 0.0696 - acc: 0.9763 - val_loss: 0.4721 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00010: acc improved from 0.97526 to 0.97633, saving model to med_dn169_l0.h5\n",
      "Epoch 1/10\n",
      "385/384 [==============================] - 207s 539ms/step - loss: 0.1646 - acc: 0.9444 - val_loss: 0.9881 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.94440, saving model to med_dn169_l1.h5\n",
      "Epoch 2/10\n",
      "385/384 [==============================] - 179s 465ms/step - loss: 0.1822 - acc: 0.9342 - val_loss: 0.8469 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.94440\n",
      "Epoch 3/10\n",
      "385/384 [==============================] - 179s 465ms/step - loss: 0.1395 - acc: 0.9511 - val_loss: 0.8490 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00003: acc improved from 0.94440 to 0.95110, saving model to med_dn169_l1.h5\n",
      "Epoch 4/10\n",
      "385/384 [==============================] - 179s 465ms/step - loss: 0.1468 - acc: 0.9465 - val_loss: 0.7228 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.95110\n",
      "Epoch 5/10\n",
      "385/384 [==============================] - 179s 466ms/step - loss: 0.1310 - acc: 0.9552 - val_loss: 0.7505 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00005: acc improved from 0.95110 to 0.95524, saving model to med_dn169_l1.h5\n",
      "Epoch 6/10\n",
      "385/384 [==============================] - 182s 473ms/step - loss: 0.1334 - acc: 0.9503 - val_loss: 0.6893 - val_acc: 0.7811\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.95524\n",
      "Epoch 7/10\n",
      "385/384 [==============================] - 183s 475ms/step - loss: 0.1226 - acc: 0.9564 - val_loss: 0.7230 - val_acc: 0.7672\n",
      "\n",
      "Epoch 00007: acc improved from 0.95524 to 0.95641, saving model to med_dn169_l1.h5\n",
      "Epoch 8/10\n",
      "385/384 [==============================] - 188s 488ms/step - loss: 0.1299 - acc: 0.9520 - val_loss: 0.6546 - val_acc: 0.7870\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.95641\n",
      "Epoch 9/10\n",
      "385/384 [==============================] - 181s 469ms/step - loss: 0.1195 - acc: 0.9589 - val_loss: 0.6803 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00009: acc improved from 0.95641 to 0.95889, saving model to med_dn169_l1.h5\n",
      "Epoch 10/10\n",
      "385/384 [==============================] - 188s 488ms/step - loss: 0.1213 - acc: 0.9554 - val_loss: 0.6214 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95889\n",
      "Epoch 1/10\n",
      "424/423 [==============================] - 217s 512ms/step - loss: 0.1939 - acc: 0.9355 - val_loss: 0.2252 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.93547, saving model to med_dn169_l2.h5\n",
      "Epoch 2/10\n",
      "424/423 [==============================] - 190s 448ms/step - loss: 0.2580 - acc: 0.9176 - val_loss: 0.2228 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.93547\n",
      "Epoch 3/10\n",
      "424/423 [==============================] - 189s 445ms/step - loss: 0.1547 - acc: 0.9469 - val_loss: 0.2104 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00003: acc improved from 0.93547 to 0.94686, saving model to med_dn169_l2.h5\n",
      "Epoch 4/10\n",
      "424/423 [==============================] - 190s 448ms/step - loss: 0.2145 - acc: 0.9265 - val_loss: 0.2150 - val_acc: 0.9142\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.94686\n",
      "Epoch 5/10\n",
      "424/423 [==============================] - 186s 439ms/step - loss: 0.1363 - acc: 0.9533 - val_loss: 0.2097 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00005: acc improved from 0.94686 to 0.95331, saving model to med_dn169_l2.h5\n",
      "Epoch 6/10\n",
      "424/423 [==============================] - 187s 442ms/step - loss: 0.1916 - acc: 0.9318 - val_loss: 0.2230 - val_acc: 0.9115\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.95331\n",
      "Epoch 7/10\n",
      "424/423 [==============================] - 190s 448ms/step - loss: 0.1265 - acc: 0.9573 - val_loss: 0.2028 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00007: acc improved from 0.95331 to 0.95725, saving model to med_dn169_l2.h5\n",
      "Epoch 8/10\n",
      "424/423 [==============================] - 191s 450ms/step - loss: 0.1877 - acc: 0.9355 - val_loss: 0.2173 - val_acc: 0.9140\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.95725\n",
      "Epoch 9/10\n",
      "424/423 [==============================] - 191s 450ms/step - loss: 0.1247 - acc: 0.9594 - val_loss: 0.1992 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00009: acc improved from 0.95725 to 0.95935, saving model to med_dn169_l2.h5\n",
      "Epoch 10/10\n",
      "424/423 [==============================] - 191s 451ms/step - loss: 0.1750 - acc: 0.9378 - val_loss: 0.2187 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95935\n",
      "Epoch 1/10\n",
      "421/420 [==============================] - 222s 528ms/step - loss: 0.2339 - acc: 0.9168 - val_loss: 0.6529 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.91683, saving model to med_dn169_l3.h5\n",
      "Epoch 2/10\n",
      "421/420 [==============================] - 195s 462ms/step - loss: 0.2225 - acc: 0.9210 - val_loss: 0.5200 - val_acc: 0.8412\n",
      "\n",
      "Epoch 00002: acc improved from 0.91683 to 0.92098, saving model to med_dn169_l3.h5\n",
      "Epoch 3/10\n",
      "421/420 [==============================] - 194s 461ms/step - loss: 0.1798 - acc: 0.9369 - val_loss: 0.5990 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00003: acc improved from 0.92098 to 0.93687, saving model to med_dn169_l3.h5\n",
      "Epoch 4/10\n",
      "421/420 [==============================] - 195s 463ms/step - loss: 0.1661 - acc: 0.9398 - val_loss: 0.4987 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00004: acc improved from 0.93687 to 0.93983, saving model to med_dn169_l3.h5\n",
      "Epoch 5/10\n",
      "421/420 [==============================] - 193s 459ms/step - loss: 0.1571 - acc: 0.9441 - val_loss: 0.5617 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00005: acc improved from 0.93983 to 0.94411, saving model to med_dn169_l3.h5\n",
      "Epoch 6/10\n",
      "421/420 [==============================] - 194s 461ms/step - loss: 0.1637 - acc: 0.9422 - val_loss: 0.4846 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.94411\n",
      "Epoch 7/10\n",
      "421/420 [==============================] - 202s 480ms/step - loss: 0.1547 - acc: 0.9478 - val_loss: 0.5571 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00007: acc improved from 0.94411 to 0.94778, saving model to med_dn169_l3.h5\n",
      "Epoch 8/10\n",
      "421/420 [==============================] - 198s 470ms/step - loss: 0.1561 - acc: 0.9444 - val_loss: 0.4774 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.94778\n",
      "Epoch 9/10\n",
      "421/420 [==============================] - 199s 472ms/step - loss: 0.1482 - acc: 0.9489 - val_loss: 0.5428 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00009: acc improved from 0.94778 to 0.94886, saving model to med_dn169_l3.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "421/420 [==============================] - 198s 470ms/step - loss: 0.1557 - acc: 0.9449 - val_loss: 0.4795 - val_acc: 0.8546\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.94886\n",
      "Epoch 1/10\n",
      "441/440 [==============================] - 234s 531ms/step - loss: 0.2191 - acc: 0.9211 - val_loss: 0.3189 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.92110, saving model to med_dn169_l4.h5\n",
      "Epoch 2/10\n",
      "441/440 [==============================] - 207s 469ms/step - loss: 0.1883 - acc: 0.9316 - val_loss: 0.3685 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00002: acc improved from 0.92110 to 0.93170, saving model to med_dn169_l4.h5\n",
      "Epoch 3/10\n",
      "441/440 [==============================] - 206s 468ms/step - loss: 0.1825 - acc: 0.9391 - val_loss: 0.3487 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00003: acc improved from 0.93170 to 0.93906, saving model to med_dn169_l4.h5\n",
      "Epoch 4/10\n",
      "441/440 [==============================] - 208s 472ms/step - loss: 0.1623 - acc: 0.9433 - val_loss: 0.3840 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00004: acc improved from 0.93906 to 0.94348, saving model to med_dn169_l4.h5\n",
      "Epoch 5/10\n",
      "441/440 [==============================] - 208s 472ms/step - loss: 0.1748 - acc: 0.9408 - val_loss: 0.3583 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.94348\n",
      "Epoch 6/10\n",
      "441/440 [==============================] - 208s 471ms/step - loss: 0.1563 - acc: 0.9457 - val_loss: 0.3893 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00006: acc improved from 0.94348 to 0.94586, saving model to med_dn169_l4.h5\n",
      "Epoch 7/10\n",
      "441/440 [==============================] - 206s 467ms/step - loss: 0.1653 - acc: 0.9433 - val_loss: 0.3632 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.94586\n",
      "Epoch 8/10\n",
      "441/440 [==============================] - 210s 475ms/step - loss: 0.1573 - acc: 0.9450 - val_loss: 0.3904 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.94586\n",
      "Epoch 9/10\n",
      "441/440 [==============================] - 206s 467ms/step - loss: 0.1647 - acc: 0.9435 - val_loss: 0.3645 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.94586\n",
      "Epoch 10/10\n",
      "441/440 [==============================] - 210s 476ms/step - loss: 0.1519 - acc: 0.9461 - val_loss: 0.3940 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00010: acc improved from 0.94586 to 0.94600, saving model to med_dn169_l4.h5\n",
      "Epoch 1/10\n",
      "440/439 [==============================] - 228s 518ms/step - loss: 0.2838 - acc: 0.8847 - val_loss: 0.2892 - val_acc: 0.8850\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.88466, saving model to med_dn169_l5.h5\n",
      "Epoch 2/10\n",
      "440/439 [==============================] - 207s 470ms/step - loss: 0.2745 - acc: 0.8900 - val_loss: 0.2118 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00002: acc improved from 0.88466 to 0.88999, saving model to med_dn169_l5.h5\n",
      "Epoch 3/10\n",
      "440/439 [==============================] - 207s 471ms/step - loss: 0.1847 - acc: 0.9339 - val_loss: 0.1882 - val_acc: 0.9216\n",
      "\n",
      "Epoch 00003: acc improved from 0.88999 to 0.93391, saving model to med_dn169_l5.h5\n",
      "Epoch 4/10\n",
      "440/439 [==============================] - 210s 477ms/step - loss: 0.1862 - acc: 0.9299 - val_loss: 0.1543 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.93391\n",
      "Epoch 5/10\n",
      "440/439 [==============================] - 206s 469ms/step - loss: 0.1630 - acc: 0.9455 - val_loss: 0.1691 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00005: acc improved from 0.93391 to 0.94549, saving model to med_dn169_l5.h5\n",
      "Epoch 6/10\n",
      "440/439 [==============================] - 207s 470ms/step - loss: 0.1670 - acc: 0.9411 - val_loss: 0.1472 - val_acc: 0.9461\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.94549\n",
      "Epoch 7/10\n",
      "440/439 [==============================] - 204s 464ms/step - loss: 0.1571 - acc: 0.9462 - val_loss: 0.1612 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00007: acc improved from 0.94549 to 0.94624, saving model to med_dn169_l5.h5\n",
      "Epoch 8/10\n",
      "440/439 [==============================] - 206s 468ms/step - loss: 0.1603 - acc: 0.9436 - val_loss: 0.1406 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.94624\n",
      "Epoch 9/10\n",
      "440/439 [==============================] - 206s 467ms/step - loss: 0.1526 - acc: 0.9467 - val_loss: 0.1576 - val_acc: 0.9407\n",
      "\n",
      "Epoch 00009: acc improved from 0.94624 to 0.94666, saving model to med_dn169_l5.h5\n",
      "Epoch 10/10\n",
      "440/439 [==============================] - 207s 469ms/step - loss: 0.1552 - acc: 0.9455 - val_loss: 0.1379 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.94666\n",
      "Epoch 1/10\n",
      "439/438 [==============================] - 220s 502ms/step - loss: 0.3792 - acc: 0.8408 - val_loss: 0.2858 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.84083, saving model to med_dn169_l6.h5\n",
      "Epoch 2/10\n",
      "439/438 [==============================] - 202s 461ms/step - loss: 0.2274 - acc: 0.9129 - val_loss: 0.2876 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00002: acc improved from 0.84083 to 0.91288, saving model to med_dn169_l6.h5\n",
      "Epoch 3/10\n",
      "439/438 [==============================] - 192s 437ms/step - loss: 0.1985 - acc: 0.9242 - val_loss: 0.1677 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00003: acc improved from 0.91288 to 0.92415, saving model to med_dn169_l6.h5\n",
      "Epoch 4/10\n",
      "439/438 [==============================] - 195s 443ms/step - loss: 0.1619 - acc: 0.9405 - val_loss: 0.2028 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00004: acc improved from 0.92415 to 0.94045, saving model to med_dn169_l6.h5\n",
      "Epoch 5/10\n",
      "439/438 [==============================] - 193s 440ms/step - loss: 0.1533 - acc: 0.9448 - val_loss: 0.1509 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00005: acc improved from 0.94045 to 0.94480, saving model to med_dn169_l6.h5\n",
      "Epoch 6/10\n",
      "439/438 [==============================] - 195s 445ms/step - loss: 0.1536 - acc: 0.9458 - val_loss: 0.1963 - val_acc: 0.9178\n",
      "\n",
      "Epoch 00006: acc improved from 0.94480 to 0.94583, saving model to med_dn169_l6.h5\n",
      "Epoch 7/10\n",
      "439/438 [==============================] - 194s 441ms/step - loss: 0.1479 - acc: 0.9489 - val_loss: 0.1429 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00007: acc improved from 0.94583 to 0.94893, saving model to med_dn169_l6.h5\n",
      "Epoch 8/10\n",
      "439/438 [==============================] - 195s 444ms/step - loss: 0.1547 - acc: 0.9446 - val_loss: 0.1839 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.94893\n",
      "Epoch 9/10\n",
      "439/438 [==============================] - 195s 445ms/step - loss: 0.1394 - acc: 0.9512 - val_loss: 0.1413 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00009: acc improved from 0.94893 to 0.95117, saving model to med_dn169_l6.h5\n",
      "Epoch 10/10\n",
      "439/438 [==============================] - 198s 451ms/step - loss: 0.1503 - acc: 0.9471 - val_loss: 0.1786 - val_acc: 0.9276\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95117\n",
      "Epoch 1/10\n",
      "449/448 [==============================] - 219s 489ms/step - loss: 0.1982 - acc: 0.9289 - val_loss: 0.2142 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.92890, saving model to med_dn169_l7.h5\n",
      "Epoch 2/10\n",
      "449/448 [==============================] - 200s 445ms/step - loss: 0.1885 - acc: 0.9380 - val_loss: 0.2176 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00002: acc improved from 0.92890 to 0.93795, saving model to med_dn169_l7.h5\n",
      "Epoch 3/10\n",
      "449/448 [==============================] - 192s 427ms/step - loss: 0.1481 - acc: 0.9482 - val_loss: 0.2039 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00003: acc improved from 0.93795 to 0.94822, saving model to med_dn169_l7.h5\n",
      "Epoch 4/10\n",
      "449/448 [==============================] - 192s 428ms/step - loss: 0.1723 - acc: 0.9448 - val_loss: 0.2081 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.94822\n",
      "Epoch 5/10\n",
      "449/448 [==============================] - 205s 457ms/step - loss: 0.1364 - acc: 0.9519 - val_loss: 0.2058 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00005: acc improved from 0.94822 to 0.95191, saving model to med_dn169_l7.h5\n",
      "Epoch 6/10\n",
      "449/448 [==============================] - 204s 455ms/step - loss: 0.1648 - acc: 0.9454 - val_loss: 0.2049 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.95191\n",
      "Epoch 7/10\n",
      "449/448 [==============================] - 204s 454ms/step - loss: 0.1328 - acc: 0.9558 - val_loss: 0.2082 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00007: acc improved from 0.95191 to 0.95584, saving model to med_dn169_l7.h5\n",
      "Epoch 8/10\n",
      "449/448 [==============================] - 205s 457ms/step - loss: 0.1584 - acc: 0.9466 - val_loss: 0.2017 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.95584\n",
      "Epoch 9/10\n",
      "449/448 [==============================] - 192s 427ms/step - loss: 0.1299 - acc: 0.9544 - val_loss: 0.2103 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.95584\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/448 [==============================] - 192s 429ms/step - loss: 0.1616 - acc: 0.9455 - val_loss: 0.2029 - val_acc: 0.9400\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95584\n",
      "Epoch 1/10\n",
      "443/442 [==============================] - 211s 477ms/step - loss: 0.2225 - acc: 0.9185 - val_loss: 0.3420 - val_acc: 0.9055\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.91849, saving model to med_dn169_l8.h5\n",
      "Epoch 2/10\n",
      "443/442 [==============================] - 188s 424ms/step - loss: 0.1021 - acc: 0.9631 - val_loss: 0.3008 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00002: acc improved from 0.91849 to 0.96305, saving model to med_dn169_l8.h5\n",
      "Epoch 3/10\n",
      "443/442 [==============================] - 187s 421ms/step - loss: 0.1913 - acc: 0.9286 - val_loss: 0.3219 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.96305\n",
      "Epoch 4/10\n",
      "443/442 [==============================] - 186s 420ms/step - loss: 0.0960 - acc: 0.9652 - val_loss: 0.2988 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00004: acc improved from 0.96305 to 0.96513, saving model to med_dn169_l8.h5\n",
      "Epoch 5/10\n",
      "443/442 [==============================] - 186s 419ms/step - loss: 0.1799 - acc: 0.9310 - val_loss: 0.3228 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.96513\n",
      "Epoch 6/10\n",
      "443/442 [==============================] - 188s 424ms/step - loss: 0.0962 - acc: 0.9656 - val_loss: 0.2955 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00006: acc improved from 0.96513 to 0.96559, saving model to med_dn169_l8.h5\n",
      "Epoch 7/10\n",
      "443/442 [==============================] - 199s 449ms/step - loss: 0.1743 - acc: 0.9345 - val_loss: 0.3171 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.96559\n",
      "Epoch 8/10\n",
      "443/442 [==============================] - 192s 433ms/step - loss: 0.0929 - acc: 0.9656 - val_loss: 0.3037 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.96559\n",
      "Epoch 9/10\n",
      "443/442 [==============================] - 217s 491ms/step - loss: 0.1657 - acc: 0.9372 - val_loss: 0.3168 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.96559\n",
      "Epoch 10/10\n",
      "443/442 [==============================] - 187s 423ms/step - loss: 0.0953 - acc: 0.9664 - val_loss: 0.2943 - val_acc: 0.8686\n",
      "\n",
      "Epoch 00010: acc improved from 0.96559 to 0.96640, saving model to med_dn169_l8.h5\n",
      "Epoch 1/10\n",
      "422/421 [==============================] - 209s 496ms/step - loss: 0.1634 - acc: 0.9438 - val_loss: 0.5912 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.94379, saving model to med_dn169_l9.h5\n",
      "Epoch 2/10\n",
      "422/421 [==============================] - 186s 442ms/step - loss: 0.2228 - acc: 0.9309 - val_loss: 0.7428 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.94379\n",
      "Epoch 3/10\n",
      "422/421 [==============================] - 195s 461ms/step - loss: 0.1434 - acc: 0.9500 - val_loss: 0.5543 - val_acc: 0.8408\n",
      "\n",
      "Epoch 00003: acc improved from 0.94379 to 0.95001, saving model to med_dn169_l9.h5\n",
      "Epoch 4/10\n",
      "422/421 [==============================] - 194s 459ms/step - loss: 0.1990 - acc: 0.9353 - val_loss: 0.6977 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.95001\n",
      "Epoch 5/10\n",
      "422/421 [==============================] - 194s 460ms/step - loss: 0.1364 - acc: 0.9530 - val_loss: 0.5356 - val_acc: 0.8440\n",
      "\n",
      "Epoch 00005: acc improved from 0.95001 to 0.95301, saving model to med_dn169_l9.h5\n",
      "Epoch 6/10\n",
      "422/421 [==============================] - 196s 465ms/step - loss: 0.1904 - acc: 0.9385 - val_loss: 0.6840 - val_acc: 0.8397\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.95301\n",
      "Epoch 7/10\n",
      "422/421 [==============================] - 194s 459ms/step - loss: 0.1262 - acc: 0.9556 - val_loss: 0.5335 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00007: acc improved from 0.95301 to 0.95564, saving model to med_dn169_l9.h5\n",
      "Epoch 8/10\n",
      "422/421 [==============================] - 195s 461ms/step - loss: 0.1804 - acc: 0.9400 - val_loss: 0.6754 - val_acc: 0.8398\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.95564\n",
      "Epoch 9/10\n",
      "422/421 [==============================] - 194s 461ms/step - loss: 0.1267 - acc: 0.9557 - val_loss: 0.5309 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00009: acc improved from 0.95564 to 0.95568, saving model to med_dn169_l9.h5\n",
      "Epoch 10/10\n",
      "422/421 [==============================] - 194s 460ms/step - loss: 0.1723 - acc: 0.9421 - val_loss: 0.6592 - val_acc: 0.8431\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95568\n",
      "Epoch 1/10\n",
      "424/423 [==============================] - 242s 570ms/step - loss: 0.3535 - acc: 0.8890 - val_loss: 0.3493 - val_acc: 0.8947\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.88897, saving model to med_dn169_l10.h5\n",
      "Epoch 2/10\n",
      "424/423 [==============================] - 212s 500ms/step - loss: 0.3731 - acc: 0.8793 - val_loss: 0.3762 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.88897\n",
      "Epoch 3/10\n",
      "424/423 [==============================] - 214s 504ms/step - loss: 0.2478 - acc: 0.9187 - val_loss: 0.3156 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00003: acc improved from 0.88897 to 0.91867, saving model to med_dn169_l10.h5\n",
      "Epoch 4/10\n",
      "424/423 [==============================] - 214s 505ms/step - loss: 0.3498 - acc: 0.8839 - val_loss: 0.4065 - val_acc: 0.8752\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.91867\n",
      "Epoch 5/10\n",
      "424/423 [==============================] - 211s 497ms/step - loss: 0.2250 - acc: 0.9250 - val_loss: 0.3181 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00005: acc improved from 0.91867 to 0.92501, saving model to med_dn169_l10.h5\n",
      "Epoch 6/10\n",
      "424/423 [==============================] - 212s 500ms/step - loss: 0.3335 - acc: 0.8852 - val_loss: 0.4069 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.92501\n",
      "Epoch 7/10\n",
      "424/423 [==============================] - 210s 495ms/step - loss: 0.2106 - acc: 0.9301 - val_loss: 0.3229 - val_acc: 0.8901\n",
      "\n",
      "Epoch 00007: acc improved from 0.92501 to 0.93006, saving model to med_dn169_l10.h5\n",
      "Epoch 8/10\n",
      "424/423 [==============================] - 212s 501ms/step - loss: 0.3348 - acc: 0.8840 - val_loss: 0.4193 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.93006\n",
      "Epoch 9/10\n",
      "424/423 [==============================] - 222s 524ms/step - loss: 0.2005 - acc: 0.9332 - val_loss: 0.3250 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00009: acc improved from 0.93006 to 0.93315, saving model to med_dn169_l10.h5\n",
      "Epoch 10/10\n",
      "424/423 [==============================] - 227s 534ms/step - loss: 0.3258 - acc: 0.8859 - val_loss: 0.4267 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.93315\n",
      "Epoch 1/10\n",
      "418/417 [==============================] - 240s 574ms/step - loss: 0.5244 - acc: 0.7805 - val_loss: 0.4156 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.78050, saving model to med_dn169_l11.h5\n",
      "Epoch 2/10\n",
      "418/417 [==============================] - 210s 502ms/step - loss: 0.3870 - acc: 0.8446 - val_loss: 0.4702 - val_acc: 0.8305\n",
      "\n",
      "Epoch 00002: acc improved from 0.78050 to 0.84456, saving model to med_dn169_l11.h5\n",
      "Epoch 3/10\n",
      "418/417 [==============================] - 207s 495ms/step - loss: 0.4228 - acc: 0.8332 - val_loss: 0.3487 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.84456\n",
      "Epoch 4/10\n",
      "418/417 [==============================] - 209s 501ms/step - loss: 0.2954 - acc: 0.8949 - val_loss: 0.4093 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00004: acc improved from 0.84456 to 0.89496, saving model to med_dn169_l11.h5\n",
      "Epoch 5/10\n",
      "418/417 [==============================] - 211s 506ms/step - loss: 0.3694 - acc: 0.8512 - val_loss: 0.3348 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.89496\n",
      "Epoch 6/10\n",
      "418/417 [==============================] - 211s 505ms/step - loss: 0.2717 - acc: 0.9036 - val_loss: 0.4066 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00006: acc improved from 0.89496 to 0.90350, saving model to med_dn169_l11.h5\n",
      "Epoch 7/10\n",
      "418/417 [==============================] - 208s 497ms/step - loss: 0.3601 - acc: 0.8589 - val_loss: 0.3271 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.90350\n",
      "Epoch 8/10\n",
      "418/417 [==============================] - 209s 500ms/step - loss: 0.2588 - acc: 0.9109 - val_loss: 0.3801 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00008: acc improved from 0.90350 to 0.91099, saving model to med_dn169_l11.h5\n",
      "Epoch 9/10\n",
      "418/417 [==============================] - 208s 498ms/step - loss: 0.3393 - acc: 0.8639 - val_loss: 0.3243 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.91099\n",
      "Epoch 10/10\n",
      "418/417 [==============================] - 210s 503ms/step - loss: 0.2504 - acc: 0.9141 - val_loss: 0.3792 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00010: acc improved from 0.91099 to 0.91410, saving model to med_dn169_l11.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "445/444 [==============================] - 218s 489ms/step - loss: 0.3977 - acc: 0.8340 - val_loss: 0.2548 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.83402, saving model to med_dn169_l12.h5\n",
      "Epoch 2/10\n",
      "445/444 [==============================] - 193s 435ms/step - loss: 0.3318 - acc: 0.8685 - val_loss: 0.2453 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00002: acc improved from 0.83402 to 0.86856, saving model to med_dn169_l12.h5\n",
      "Epoch 3/10\n",
      "445/444 [==============================] - 199s 447ms/step - loss: 0.2644 - acc: 0.8989 - val_loss: 0.1920 - val_acc: 0.9450\n",
      "\n",
      "Epoch 00003: acc improved from 0.86856 to 0.89891, saving model to med_dn169_l12.h5\n",
      "Epoch 4/10\n",
      "445/444 [==============================] - 206s 463ms/step - loss: 0.2702 - acc: 0.8956 - val_loss: 0.2020 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.89891\n",
      "Epoch 5/10\n",
      "445/444 [==============================] - 197s 444ms/step - loss: 0.2192 - acc: 0.9221 - val_loss: 0.1876 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00005: acc improved from 0.89891 to 0.92209, saving model to med_dn169_l12.h5\n",
      "Epoch 6/10\n",
      "445/444 [==============================] - 193s 434ms/step - loss: 0.2574 - acc: 0.8991 - val_loss: 0.1960 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.92209\n",
      "Epoch 7/10\n",
      "445/444 [==============================] - 191s 430ms/step - loss: 0.1972 - acc: 0.9313 - val_loss: 0.1726 - val_acc: 0.9503\n",
      "\n",
      "Epoch 00007: acc improved from 0.92209 to 0.93129, saving model to med_dn169_l12.h5\n",
      "Epoch 8/10\n",
      "445/444 [==============================] - 191s 429ms/step - loss: 0.2521 - acc: 0.9009 - val_loss: 0.1891 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.93129\n",
      "Epoch 9/10\n",
      "445/444 [==============================] - 191s 429ms/step - loss: 0.1849 - acc: 0.9364 - val_loss: 0.1679 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00009: acc improved from 0.93129 to 0.93645, saving model to med_dn169_l12.h5\n",
      "Epoch 10/10\n",
      "445/444 [==============================] - 192s 432ms/step - loss: 0.2390 - acc: 0.9063 - val_loss: 0.1874 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.93645\n",
      "Epoch 1/10\n",
      "439/438 [==============================] - 230s 525ms/step - loss: 0.3307 - acc: 0.8639 - val_loss: 0.3224 - val_acc: 0.8910\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.86390, saving model to med_dn169_l13.h5\n",
      "Epoch 2/10\n",
      "439/438 [==============================] - 202s 459ms/step - loss: 0.4045 - acc: 0.8420 - val_loss: 0.4729 - val_acc: 0.8857\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.86390\n",
      "Epoch 3/10\n",
      "439/438 [==============================] - 229s 522ms/step - loss: 0.2520 - acc: 0.9049 - val_loss: 0.2235 - val_acc: 0.9418\n",
      "\n",
      "Epoch 00003: acc improved from 0.86390 to 0.90493, saving model to med_dn169_l13.h5\n",
      "Epoch 4/10\n",
      "439/438 [==============================] - 214s 487ms/step - loss: 0.3144 - acc: 0.8748 - val_loss: 0.3603 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.90493\n",
      "Epoch 5/10\n",
      "439/438 [==============================] - 199s 454ms/step - loss: 0.2195 - acc: 0.9188 - val_loss: 0.2077 - val_acc: 0.9489\n",
      "\n",
      "Epoch 00005: acc improved from 0.90493 to 0.91881, saving model to med_dn169_l13.h5\n",
      "Epoch 6/10\n",
      "439/438 [==============================] - 202s 461ms/step - loss: 0.2978 - acc: 0.8827 - val_loss: 0.3460 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.91881\n",
      "Epoch 7/10\n",
      "439/438 [==============================] - 200s 455ms/step - loss: 0.2107 - acc: 0.9252 - val_loss: 0.1894 - val_acc: 0.9533\n",
      "\n",
      "Epoch 00007: acc improved from 0.91881 to 0.92522, saving model to med_dn169_l13.h5\n",
      "Epoch 8/10\n",
      "439/438 [==============================] - 204s 464ms/step - loss: 0.2832 - acc: 0.8879 - val_loss: 0.3277 - val_acc: 0.9374\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.92522\n",
      "Epoch 9/10\n",
      "439/438 [==============================] - 214s 488ms/step - loss: 0.1995 - acc: 0.9280 - val_loss: 0.1917 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00009: acc improved from 0.92522 to 0.92803, saving model to med_dn169_l13.h5\n",
      "Epoch 10/10\n",
      "439/438 [==============================] - 217s 494ms/step - loss: 0.2797 - acc: 0.8905 - val_loss: 0.3197 - val_acc: 0.9409\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.92803\n",
      "Epoch 1/10\n",
      "428/427 [==============================] - 253s 591ms/step - loss: 0.3982 - acc: 0.8299 - val_loss: 0.5266 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.82991, saving model to med_dn169_l14.h5\n",
      "Epoch 2/10\n",
      "428/427 [==============================] - 216s 505ms/step - loss: 0.4056 - acc: 0.8267 - val_loss: 0.4540 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.82991\n",
      "Epoch 3/10\n",
      "428/427 [==============================] - 220s 515ms/step - loss: 0.3151 - acc: 0.8731 - val_loss: 0.4595 - val_acc: 0.9038\n",
      "\n",
      "Epoch 00003: acc improved from 0.82991 to 0.87307, saving model to med_dn169_l14.h5\n",
      "Epoch 4/10\n",
      "428/427 [==============================] - 217s 507ms/step - loss: 0.3112 - acc: 0.8786 - val_loss: 0.4132 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00004: acc improved from 0.87307 to 0.87855, saving model to med_dn169_l14.h5\n",
      "Epoch 5/10\n",
      "428/427 [==============================] - 215s 502ms/step - loss: 0.2918 - acc: 0.8855 - val_loss: 0.4427 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00005: acc improved from 0.87855 to 0.88551, saving model to med_dn169_l14.h5\n",
      "Epoch 6/10\n",
      "428/427 [==============================] - 214s 501ms/step - loss: 0.2843 - acc: 0.8914 - val_loss: 0.3541 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00006: acc improved from 0.88551 to 0.89135, saving model to med_dn169_l14.h5\n",
      "Epoch 7/10\n",
      "428/427 [==============================] - 212s 496ms/step - loss: 0.2777 - acc: 0.8908 - val_loss: 0.4222 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.89135\n",
      "Epoch 8/10\n",
      "428/427 [==============================] - 215s 503ms/step - loss: 0.2713 - acc: 0.8979 - val_loss: 0.3493 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00008: acc improved from 0.89135 to 0.89796, saving model to med_dn169_l14.h5\n",
      "Epoch 9/10\n",
      "428/427 [==============================] - 215s 502ms/step - loss: 0.2788 - acc: 0.8923 - val_loss: 0.4155 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.89796\n",
      "Epoch 10/10\n",
      "428/427 [==============================] - 213s 497ms/step - loss: 0.2625 - acc: 0.8991 - val_loss: 0.3566 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00010: acc improved from 0.89796 to 0.89910, saving model to med_dn169_l14.h5\n",
      "Epoch 1/10\n",
      "441/440 [==============================] - 244s 553ms/step - loss: 0.2965 - acc: 0.8836 - val_loss: 0.2827 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.88365, saving model to med_dn169_l15.h5\n",
      "Epoch 2/10\n",
      "441/440 [==============================] - 204s 463ms/step - loss: 0.3271 - acc: 0.8746 - val_loss: 0.2736 - val_acc: 0.9006\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.88365\n",
      "Epoch 3/10\n",
      "441/440 [==============================] - 229s 519ms/step - loss: 0.2298 - acc: 0.9155 - val_loss: 0.2422 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00003: acc improved from 0.88365 to 0.91546, saving model to med_dn169_l15.h5\n",
      "Epoch 4/10\n",
      "441/440 [==============================] - 220s 499ms/step - loss: 0.2795 - acc: 0.8930 - val_loss: 0.2230 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.91546\n",
      "Epoch 5/10\n",
      "441/440 [==============================] - 206s 467ms/step - loss: 0.2090 - acc: 0.9280 - val_loss: 0.2369 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00005: acc improved from 0.91546 to 0.92804, saving model to med_dn169_l15.h5\n",
      "Epoch 6/10\n",
      "441/440 [==============================] - 208s 471ms/step - loss: 0.2714 - acc: 0.8966 - val_loss: 0.2139 - val_acc: 0.9278\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.92804\n",
      "Epoch 7/10\n",
      "441/440 [==============================] - 204s 463ms/step - loss: 0.1979 - acc: 0.9310 - val_loss: 0.2390 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00007: acc improved from 0.92804 to 0.93098, saving model to med_dn169_l15.h5\n",
      "Epoch 8/10\n",
      "441/440 [==============================] - 204s 462ms/step - loss: 0.2656 - acc: 0.8962 - val_loss: 0.2036 - val_acc: 0.9323\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.93098\n",
      "Epoch 9/10\n",
      "441/440 [==============================] - 201s 456ms/step - loss: 0.1926 - acc: 0.9325 - val_loss: 0.2415 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00009: acc improved from 0.93098 to 0.93247, saving model to med_dn169_l15.h5\n",
      "Epoch 10/10\n",
      "441/440 [==============================] - 204s 463ms/step - loss: 0.2615 - acc: 0.8968 - val_loss: 0.2003 - val_acc: 0.9337\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.93247\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/409 [==============================] - 223s 543ms/step - loss: 0.3286 - acc: 0.9010 - val_loss: 0.8757 - val_acc: 0.7716\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.90095, saving model to med_dn169_l16.h5\n",
      "Epoch 2/10\n",
      "410/409 [==============================] - 195s 477ms/step - loss: 0.1479 - acc: 0.9468 - val_loss: 1.3105 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00002: acc improved from 0.90095 to 0.94676, saving model to med_dn169_l16.h5\n",
      "Epoch 3/10\n",
      "410/409 [==============================] - 210s 512ms/step - loss: 0.2666 - acc: 0.9147 - val_loss: 0.9230 - val_acc: 0.7627\n",
      "\n",
      "Epoch 00003: acc did not improve from 0.94676\n",
      "Epoch 4/10\n",
      "410/409 [==============================] - 208s 508ms/step - loss: 0.1320 - acc: 0.9542 - val_loss: 1.3198 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00004: acc improved from 0.94676 to 0.95419, saving model to med_dn169_l16.h5\n",
      "Epoch 5/10\n",
      "410/409 [==============================] - 208s 506ms/step - loss: 0.2377 - acc: 0.9204 - val_loss: 0.9326 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00005: acc did not improve from 0.95419\n",
      "Epoch 6/10\n",
      "410/409 [==============================] - 209s 509ms/step - loss: 0.1268 - acc: 0.9543 - val_loss: 1.3275 - val_acc: 0.7141\n",
      "\n",
      "Epoch 00006: acc improved from 0.95419 to 0.95435, saving model to med_dn169_l16.h5\n",
      "Epoch 7/10\n",
      "410/409 [==============================] - 204s 498ms/step - loss: 0.2234 - acc: 0.9235 - val_loss: 0.9336 - val_acc: 0.7537\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.95435\n",
      "Epoch 8/10\n",
      "410/409 [==============================] - 205s 501ms/step - loss: 0.1228 - acc: 0.9564 - val_loss: 1.3205 - val_acc: 0.7127\n",
      "\n",
      "Epoch 00008: acc improved from 0.95435 to 0.95637, saving model to med_dn169_l16.h5\n",
      "Epoch 9/10\n",
      "410/409 [==============================] - 204s 498ms/step - loss: 0.2076 - acc: 0.9282 - val_loss: 0.9403 - val_acc: 0.7507\n",
      "\n",
      "Epoch 00009: acc did not improve from 0.95637\n",
      "Epoch 10/10\n",
      "410/409 [==============================] - 205s 500ms/step - loss: 0.1192 - acc: 0.9561 - val_loss: 1.3131 - val_acc: 0.7128\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95637\n",
      "Epoch 1/10\n",
      "424/423 [==============================] - 244s 576ms/step - loss: 0.2183 - acc: 0.9299 - val_loss: 1.2996 - val_acc: 0.7078\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.92991, saving model to med_dn169_l17.h5\n",
      "Epoch 2/10\n",
      "424/423 [==============================] - 210s 496ms/step - loss: 0.3378 - acc: 0.8908 - val_loss: 0.9220 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.92991\n",
      "Epoch 3/10\n",
      "424/423 [==============================] - 212s 499ms/step - loss: 0.1466 - acc: 0.9517 - val_loss: 1.2503 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00003: acc improved from 0.92991 to 0.95165, saving model to med_dn169_l17.h5\n",
      "Epoch 4/10\n",
      "424/423 [==============================] - 210s 496ms/step - loss: 0.2353 - acc: 0.9196 - val_loss: 0.8131 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.95165\n",
      "Epoch 5/10\n",
      "424/423 [==============================] - 209s 492ms/step - loss: 0.1250 - acc: 0.9578 - val_loss: 1.2031 - val_acc: 0.7449\n",
      "\n",
      "Epoch 00005: acc improved from 0.95165 to 0.95781, saving model to med_dn169_l17.h5\n",
      "Epoch 6/10\n",
      "424/423 [==============================] - 211s 498ms/step - loss: 0.2188 - acc: 0.9254 - val_loss: 0.8005 - val_acc: 0.7957\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.95781\n",
      "Epoch 7/10\n",
      "424/423 [==============================] - 209s 493ms/step - loss: 0.1155 - acc: 0.9621 - val_loss: 1.2073 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00007: acc improved from 0.95781 to 0.96212, saving model to med_dn169_l17.h5\n",
      "Epoch 8/10\n",
      "424/423 [==============================] - 210s 496ms/step - loss: 0.1968 - acc: 0.9325 - val_loss: 0.7722 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.96212\n",
      "Epoch 9/10\n",
      "424/423 [==============================] - 216s 509ms/step - loss: 0.1121 - acc: 0.9622 - val_loss: 1.1998 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00009: acc improved from 0.96212 to 0.96223, saving model to med_dn169_l17.h5\n",
      "Epoch 10/10\n",
      "424/423 [==============================] - 212s 500ms/step - loss: 0.1892 - acc: 0.9328 - val_loss: 0.7482 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.96223\n",
      "Epoch 1/10\n",
      "408/407 [==============================] - 239s 585ms/step - loss: 0.1674 - acc: 0.9407 - val_loss: 0.3529 - val_acc: 0.8980\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.94072, saving model to med_dn169_l18.h5\n",
      "Epoch 2/10\n",
      "408/407 [==============================] - 217s 531ms/step - loss: 0.1697 - acc: 0.9406 - val_loss: 0.2711 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.94072\n",
      "Epoch 3/10\n",
      "408/407 [==============================] - 213s 522ms/step - loss: 0.1438 - acc: 0.9488 - val_loss: 0.2616 - val_acc: 0.9163\n",
      "\n",
      "Epoch 00003: acc improved from 0.94072 to 0.94876, saving model to med_dn169_l18.h5\n",
      "Epoch 4/10\n",
      "408/407 [==============================] - 215s 528ms/step - loss: 0.1520 - acc: 0.9474 - val_loss: 0.2458 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.94876\n",
      "Epoch 5/10\n",
      "408/407 [==============================] - 216s 530ms/step - loss: 0.1317 - acc: 0.9516 - val_loss: 0.2578 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00005: acc improved from 0.94876 to 0.95159, saving model to med_dn169_l18.h5\n",
      "Epoch 6/10\n",
      "408/407 [==============================] - 214s 525ms/step - loss: 0.1443 - acc: 0.9483 - val_loss: 0.2403 - val_acc: 0.9052\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.95159\n",
      "Epoch 7/10\n",
      "408/407 [==============================] - 211s 517ms/step - loss: 0.1223 - acc: 0.9550 - val_loss: 0.2426 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00007: acc improved from 0.95159 to 0.95504, saving model to med_dn169_l18.h5\n",
      "Epoch 8/10\n",
      "408/407 [==============================] - 214s 524ms/step - loss: 0.1399 - acc: 0.9489 - val_loss: 0.2383 - val_acc: 0.9039\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.95504\n",
      "Epoch 9/10\n",
      "408/407 [==============================] - 211s 518ms/step - loss: 0.1188 - acc: 0.9562 - val_loss: 0.2402 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00009: acc improved from 0.95504 to 0.95623, saving model to med_dn169_l18.h5\n",
      "Epoch 10/10\n",
      "408/407 [==============================] - 215s 526ms/step - loss: 0.1306 - acc: 0.9523 - val_loss: 0.2363 - val_acc: 0.9047\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.95623\n",
      "Epoch 1/10\n",
      "419/418 [==============================] - 251s 598ms/step - loss: 0.1013 - acc: 0.9640 - val_loss: 0.3998 - val_acc: 0.8776\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.96401, saving model to med_dn169_l19.h5\n",
      "Epoch 2/10\n",
      "419/418 [==============================] - 216s 517ms/step - loss: 0.1471 - acc: 0.9453 - val_loss: 0.3438 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00002: acc did not improve from 0.96401\n",
      "Epoch 3/10\n",
      "419/418 [==============================] - 220s 526ms/step - loss: 0.0973 - acc: 0.9657 - val_loss: 0.3863 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00003: acc improved from 0.96401 to 0.96565, saving model to med_dn169_l19.h5\n",
      "Epoch 4/10\n",
      "419/418 [==============================] - 218s 520ms/step - loss: 0.1360 - acc: 0.9486 - val_loss: 0.3424 - val_acc: 0.8824\n",
      "\n",
      "Epoch 00004: acc did not improve from 0.96565\n",
      "Epoch 5/10\n",
      "419/418 [==============================] - 216s 515ms/step - loss: 0.0958 - acc: 0.9658 - val_loss: 0.3860 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00005: acc improved from 0.96565 to 0.96584, saving model to med_dn169_l19.h5\n",
      "Epoch 6/10\n",
      "419/418 [==============================] - 211s 504ms/step - loss: 0.1327 - acc: 0.9506 - val_loss: 0.3443 - val_acc: 0.8815\n",
      "\n",
      "Epoch 00006: acc did not improve from 0.96584\n",
      "Epoch 7/10\n",
      "419/418 [==============================] - 214s 511ms/step - loss: 0.0973 - acc: 0.9661 - val_loss: 0.3789 - val_acc: 0.8816\n",
      "\n",
      "Epoch 00007: acc improved from 0.96584 to 0.96610, saving model to med_dn169_l19.h5\n",
      "Epoch 8/10\n",
      "419/418 [==============================] - 215s 513ms/step - loss: 0.1316 - acc: 0.9512 - val_loss: 0.3375 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00008: acc did not improve from 0.96610\n",
      "Epoch 9/10\n",
      "419/418 [==============================] - 221s 527ms/step - loss: 0.0940 - acc: 0.9674 - val_loss: 0.3790 - val_acc: 0.8814\n",
      "\n",
      "Epoch 00009: acc improved from 0.96610 to 0.96741, saving model to med_dn169_l19.h5\n",
      "Epoch 10/10\n",
      "419/418 [==============================] - 224s 535ms/step - loss: 0.1322 - acc: 0.9499 - val_loss: 0.3454 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00010: acc did not improve from 0.96741\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    for i in range (40, 60):\n",
    "        with open(r\"med_dn169_{0}.yaml\".format(i), \"r\") as yaml_file:\n",
    "            K.clear_session()\n",
    "            indep, dep, indep_val, dep_val, res = None, None, None, None, None\n",
    "            indep, dep, indep_val, dep_val = getfold(i)\n",
    "            gs1 = model_from_yaml(yaml_file.read())\n",
    "            gs1.load_weights(\"med_dn169_{0}.h5\".format(i))\n",
    "            rn = gs1.layers[1]\n",
    "            \n",
    "#            rn.Trainable = False\n",
    "#            set_trainable = False\n",
    "#            for layer in rn.layers:\n",
    "#                if layer.name == 'res5a_branch2a':\n",
    "#                    set_trainable = True\n",
    "#                if set_trainable:\n",
    "#                    layer.trainable = True\n",
    "#                else:\n",
    "#                    layer.trainable = False\n",
    "            rn.Trainable = True\n",
    "            for layer in rn.layers:\n",
    "                layer.trainable = True\n",
    "\n",
    "\n",
    "            gs1.compile(RMSprop(lr=0.00001), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "            clr_triangular = CyclicLR(base_lr=1e-6, max_lr=0.00001, mode=\"triangular2\", step_size=1000)\n",
    "        \n",
    "            with open(r\"med_dn169_5_l{0}.yaml\".format(i-40), \"w\") as yaml_file:\n",
    "                yaml_file.write(gs1.to_yaml())\n",
    "            gs1.fit_generator(generator=NPGenerator(indep=indep, dep=dep, batch_size=64, transform=preprocess_train), \n",
    "                          validation_data=NPGenerator(indep=indep_val, dep=dep_val, batch_size=64, transform=preprocess_val), \n",
    "                          steps_per_epoch=indep.shape[0]/2/64,\n",
    "                          validation_steps=indep_val.shape[0]/64,\n",
    "                          epochs=10, verbose=1, callbacks = \n",
    "                          [\n",
    "                              ModelCheckpoint(\"med_dn169_l{0}.h5\".format(i-40), monitor='acc', verbose=1, save_best_only=True, mode='max'),\n",
    "                              clr_triangular\n",
    "                          ])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJMAAAJaCAYAAACbRAbCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMcokwAAAAAAAODYfwG+ELRJF7jSLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1daadc25710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex='col', figsize=(20, 10))\n",
    "ax[0].set_title('Model accuracy history')\n",
    "ax[0].plot(gs1.history.history['acc'])\n",
    "ax[0].plot(gs1.history.history['val_acc'])\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].legend(['train', 'test'], loc='right')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].set_title('Model loss history')\n",
    "ax[1].plot(gs1.history.history['loss'])\n",
    "ax[1].plot(gs1.history.history['val_loss'])\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend(['train', 'test'], loc='right')\n",
    "ax[1].grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    for i in range (0, 9):\n",
    "        with open(r\"med_dn169_lll{0}.yaml\".format(i), \"r\") as yaml_file:\n",
    "            K.clear_session()\n",
    "            indep, dep, indep_val, dep_val, res = None, None, None, None, None\n",
    "            indep, dep, indep_val, dep_val = getfold(i+10)\n",
    "            gs1 = model_from_yaml(yaml_file.read())\n",
    "            gs1.load_weights(\"med_dn169_lll{0}.h5\".format(i))\n",
    "            rn = gs1.layers[1]\n",
    "            rn.Trainable = True\n",
    "            for layer in rn.layers:\n",
    "                layer.trainable = True\n",
    "\n",
    "\n",
    "            gs1.compile(RMSprop(lr=0.001), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "            clr_triangular = CyclicLR(base_lr=1e-5, max_lr=0.0001, mode=\"triangular2\", step_size=2000)\n",
    "        \n",
    "            with open(r\"med_dn169_llll{0}.yaml\".format(i), \"w\") as yaml_file:\n",
    "                yaml_file.write(gs1.to_yaml())\n",
    "            gs1.fit_generator(generator=NPGenerator(indep=indep, dep=dep, batch_size=64, transform=preprocess_train), \n",
    "                          validation_data=NPGenerator(indep=indep_val, dep=dep_val, batch_size=64, transform=preprocess_val), \n",
    "                          steps_per_epoch=indep.shape[0]/64,\n",
    "                          validation_steps=indep_val.shape[0]/64,\n",
    "                          epochs=5, verbose=1, callbacks = \n",
    "                          [\n",
    "                              ModelCheckpoint(\"med_dn169_llll{0}.h5\".format(i), monitor='acc', verbose=1, save_best_only=True, mode='max'),\n",
    "                          ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "90/90 [==============================] - 90s 995ms/step\n",
      "0 1\n",
      "90/90 [==============================] - 85s 947ms/step\n",
      "0 2\n",
      "90/90 [==============================] - 85s 941ms/step\n",
      "0 3\n",
      "90/90 [==============================] - 87s 967ms/step\n",
      "1 0\n",
      "90/90 [==============================] - 85s 939ms/step\n",
      "1 1\n",
      "90/90 [==============================] - 80s 891ms/step\n",
      "1 2\n",
      "90/90 [==============================] - 80s 894ms/step\n",
      "1 3\n",
      "90/90 [==============================] - 82s 913ms/step\n",
      "2 0\n",
      "90/90 [==============================] - 82s 911ms/step\n",
      "2 1\n",
      "90/90 [==============================] - 83s 920ms/step\n",
      "2 2\n",
      "90/90 [==============================] - 81s 901ms/step\n",
      "2 3\n",
      "90/90 [==============================] - 81s 900ms/step\n",
      "3 0\n",
      "90/90 [==============================] - 82s 911ms/step\n",
      "3 1\n",
      "69/90 [======================>.......] - ETA: 19s"
     ]
    }
   ],
   "source": [
    "res = np.load(\"res.npy\")\n",
    "data = pd.DataFrame()\n",
    "model =[]\n",
    "for i in range(0, 9):\n",
    "    with open(r\"med_dn169_lll{0}.yaml\".format(i), \"r\") as yaml_file:\n",
    "        K.clear_session()\n",
    "        m = model_from_yaml(yaml_file.read())\n",
    "        m.load_weights(r\"med_dn169_lll{0}.h5\".format(i))\n",
    "        for j in range(0,4):\n",
    "            print (i, j)\n",
    "            tg = NPGenerator(indep=res, batch_size=640, transform=preprocess_test)\n",
    "            a = m.predict_generator(tg ,verbose=1, steps=len(tg))\n",
    "            data[\"model_{0}\".format(i * 10 + j)] = pd.Series(a.reshape(len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = ImageDataGenerator().flow_from_directory(directory=r\"c:/users/ajaln/test/\",\n",
    "                                                    target_size=(96, 96),\n",
    "                                                    color_mode=\"rgb\", batch_size=1000,\n",
    "                                                    class_mode=\"binary\", shuffle=False)\n",
    "result = pd.DataFrame()\n",
    "result[\"id\"] = test_generator.filenames\n",
    "result[\"id\"] = result[\"id\"].str[5:45]\n",
    "result[\"label\"] = data.iloc[:,:].mean(axis=1)\n",
    "result.head()\n",
    "samples = pd.read_csv(r\"c:/users/ajaln/sample_submission.csv\", usecols=[\"id\"])\n",
    "samples = pd.merge(samples, result, on=\"id\", how=\"inner\")\n",
    "samples.head()\n",
    "samples.to_csv(r\"c:/work/dataset/medical/medical_dn169_tl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(r\"c:/work/dataset/medical/medical_dn169_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2 = pd.read_csv(r\"C:/Work/dataset/medical/medical_dn169_tl.csv\")\n",
    "a = pd.read_csv(r\"C:/Work/dataset/medical/kl2.csv\")\n",
    "\n",
    "a[\"label\"] = (a2[\"label\"]*0.5+a[\"label\"]*0.5)\n",
    "a.loc[:, [\"id\", \"label\"]].to_csv(r\"c:/work/dataset/medical/last.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [y+x*4 for x in [2,5,6,7,12,13,14,15] for y in [0,1,2,3]]\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
