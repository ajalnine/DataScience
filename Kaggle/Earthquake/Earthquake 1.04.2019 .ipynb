{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 603156128222672947\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4945621811\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4115183845581934969\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:08:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import tqdm\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, model_from_yaml, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "config = tf.ConfigProto(device_count={\"CPU\": 1, \"GPU\" : 1})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"c:\\users\\ajaln\\train\\train.csv\", usecols=[\"acoustic_data\", \"time_to_failure\"], \n",
    "                    dtype={\"acoustic_data\":np.int16, \"time_to_failure\":np.float64})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████▉| 38391/38398 [1:20:39<00:00,  8.02it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 34384 into shape (1,7,5000,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e9e596ba5568>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0msingle_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m150000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcwt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acoustic_data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mexh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time_to_failure\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 34384 into shape (1,7,5000,1)"
     ]
    }
   ],
   "source": [
    "samples = 9600*4-8\n",
    "img = np.empty([samples,7,5000,1])\n",
    "dep = np.empty([samples])\n",
    "with tqdm.tqdm(total=samples) as bar:\n",
    "    for a in range(0, samples):\n",
    "        offset = 4096*a*4\n",
    "        single_sample = train.iloc[offset:offset+150000]\n",
    "        w1 = pywt.cwt(single_sample[\"acoustic_data\"], [1, 5, 10, 15, 25, 50, 100], \"mexh\")[0][:,::30]\n",
    "        img[a] = w1.reshape(1,7,5000)\n",
    "        dep[a] = single_sample[\"time_to_failure\"].min()\n",
    "        bar.update(1)\n",
    "np.save(\"indep.npy\", img)\n",
    "np.save(\"dep.npy\", dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = np.load(\"indep.npy\")\n",
    "dep =np.load(\"dep.npy\")\n",
    "img = img.reshape(img.shape[0],7,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    " \n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# Code is ported from https://github.com/fastai/fastai\n",
    "class OneCycleLR(Callback):\n",
    "    def __init__(self,\n",
    "                 max_lr,\n",
    "                 end_percentage=0.1,\n",
    "                 scale_percentage=None,\n",
    "                 maximum_momentum=0.95,\n",
    "                 minimum_momentum=0.85,\n",
    "                 verbose=True):\n",
    "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
    "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
    "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
    "        100th its initial lowest value.\n",
    "\n",
    "        # Arguments:\n",
    "            max_lr: Float. Initial learning rate. This also sets the\n",
    "                starting learning rate (which will be 10x smaller than\n",
    "                this), and will increase to this value during the first cycle.\n",
    "            end_percentage: Float. The percentage of all the epochs of training\n",
    "                that will be dedicated to sharply decreasing the learning\n",
    "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
    "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
    "                If None, it will compute the scale_percentage automatically\n",
    "                based on the `end_percentage`.\n",
    "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
    "                value, which gradually drops to its lowest value in half-cycle,\n",
    "                then gradually increases again to stay constant at this max value.\n",
    "                Can only be used with SGD Optimizer.\n",
    "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
    "                the half-cycle. Can only be used with SGD Optimizer.\n",
    "            verbose: Bool. Whether to print the current learning rate after every\n",
    "                epoch.\n",
    "\n",
    "        # Reference\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
    "        \"\"\"\n",
    "        super(OneCycleLR, self).__init__()\n",
    "\n",
    "        if end_percentage < 0. or end_percentage > 1.:\n",
    "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
    "\n",
    "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
    "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
    "\n",
    "        self.initial_lr = max_lr\n",
    "        self.end_percentage = end_percentage\n",
    "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
    "        self.max_momentum = maximum_momentum\n",
    "        self.min_momentum = minimum_momentum\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.max_momentum is not None and self.min_momentum is not None:\n",
    "            self._update_momentum = True\n",
    "        else:\n",
    "            self._update_momentum = False\n",
    "\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self.epochs = None\n",
    "        self.batch_size = None\n",
    "        self.samples = None\n",
    "        self.steps = None\n",
    "        self.num_iterations = None\n",
    "        self.mid_cycle_id = None\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the callback.\n",
    "        \"\"\"\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "    def compute_lr(self):\n",
    "        \"\"\"\n",
    "        Compute the learning rate based on which phase of the cycle it is in.\n",
    "\n",
    "        - If in the first half of training, the learning rate gradually increases.\n",
    "        - If in the second half of training, the learning rate gradually decreases.\n",
    "        - If in the final `end_percentage` portion of training, the learning rate\n",
    "            is quickly reduced to near 100th of the original min learning rate.\n",
    "\n",
    "        # Returns:\n",
    "            the new learning rate\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
    "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
    "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
    "                                              (1. - 100.) / 100.)) * self.scale\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - (\n",
    "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage *\n",
    "                                        (self.scale * 100 - 1.)) * self.scale\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage *\n",
    "                                        (self.scale * 100 - 1.)) * self.scale\n",
    "\n",
    "        if self.clr_iterations == self.num_iterations:\n",
    "            self.clr_iterations = 0\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    def compute_momentum(self):\n",
    "        \"\"\"\n",
    "         Compute the momentum based on which phase of the cycle it is in.\n",
    "\n",
    "        - If in the first half of training, the momentum gradually decreases.\n",
    "        - If in the second half of training, the momentum gradually increases.\n",
    "        - If in the final `end_percentage` portion of training, the momentum value\n",
    "            is kept constant at the maximum initial value.\n",
    "\n",
    "        # Returns:\n",
    "            the new momentum value\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            new_momentum = self.max_momentum\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
    "                                        self.mid_cycle_id))\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                self.max_momentum - self.min_momentum)\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                self.max_momentum - self.min_momentum)\n",
    "\n",
    "        return new_momentum\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.batch_size = self.params['batch_size']\n",
    "        self.samples = self.params['samples']\n",
    "        self.steps = self.params['steps']\n",
    "\n",
    "        if self.steps is not None:\n",
    "            self.num_iterations = self.epochs * self.steps\n",
    "        else:\n",
    "            if (self.samples % self.batch_size) == 0:\n",
    "                remainder = 0\n",
    "            else:\n",
    "                remainder = 1\n",
    "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
    "\n",
    "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
    "\n",
    "        self._reset()\n",
    "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        self.clr_iterations += 1\n",
    "        new_lr = self.compute_lr()\n",
    "\n",
    "        self.history.setdefault('lr', []).append(\n",
    "            K.get_value(self.model.optimizer.lr))\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "\n",
    "            self.history.setdefault('momentum', []).append(\n",
    "                K.get_value(self.model.optimizer.momentum))\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.verbose:\n",
    "            if self._update_momentum:\n",
    "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
    "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
    "\n",
    "            else:\n",
    "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self,\n",
    "                 num_samples,\n",
    "                 batch_size,\n",
    "                 minimum_lr=1e-5,\n",
    "                 maximum_lr=10.,\n",
    "                 lr_scale='exp',\n",
    "                 validation_data=None,\n",
    "                 validation_sample_rate=5,\n",
    "                 stopping_criterion_factor=4.,\n",
    "                 loss_smoothing_beta=0.98,\n",
    "                 save_dir=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        This class uses the Cyclic Learning Rate history to find a\n",
    "        set of learning rates that can be good initializations for the\n",
    "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
    "        below.\n",
    "\n",
    "        A port of the Fast.ai implementation for Keras.\n",
    "\n",
    "        # Note\n",
    "        This requires that the model be trained for exactly 1 epoch. If the model\n",
    "        is trained for more epochs, then the metric calculations are only done for\n",
    "        the first epoch.\n",
    "\n",
    "        # Interpretation\n",
    "        Upon visualizing the loss plot, check where the loss starts to increase\n",
    "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
    "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
    "        Choose the max value as this value when passing the `max_val` argument\n",
    "        to OneCycleLR callback.\n",
    "\n",
    "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
    "\n",
    "        # Arguments:\n",
    "            num_samples: Integer. Number of samples in the dataset.\n",
    "            batch_size: Integer. Batch size during training.\n",
    "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
    "            maximum_lr: Float. Final learning rate (and the maximum).\n",
    "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
    "                scaling for each update to the learning rate during subsequent\n",
    "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
    "            validation_data: Requires the validation dataset as a tuple of\n",
    "                (X, y) belonging to the validation set. If provided, will use the\n",
    "                validation set to compute the loss metrics. Else uses the training\n",
    "                batch loss. Will warn if not provided to alert the user.\n",
    "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
    "                validation set per iteration of the LRFinder. Larger number of\n",
    "                samples will reduce the variance but will take longer time to execute\n",
    "                per batch.\n",
    "\n",
    "                If Positive > 0, will sample from the validation dataset\n",
    "                If Megative, will use the entire dataset\n",
    "            stopping_criterion_factor: Integer or None. A factor which is used\n",
    "                to measure large increase in the loss value during training.\n",
    "                Since callbacks cannot stop training of a model, it will simply\n",
    "                stop logging the additional values from the epochs after this\n",
    "                stopping criterion has been met.\n",
    "                If None, this check will not be performed.\n",
    "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
    "                average of the loss function.\n",
    "            save_dir: Optional, String. If passed a directory path, the callback\n",
    "                will save the running loss and learning rates to two separate numpy\n",
    "                arrays inside this directory. If the directory in this path does not\n",
    "                exist, they will be created.\n",
    "            verbose: Whether to print the learning rate after every batch of training.\n",
    "\n",
    "        # References:\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "        \"\"\"\n",
    "        super(LRFinder, self).__init__()\n",
    "\n",
    "        if lr_scale not in ['exp', 'linear']:\n",
    "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
    "\n",
    "        if validation_data is not None:\n",
    "            self.validation_data = validation_data\n",
    "            self.use_validation_set = True\n",
    "\n",
    "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
    "                self.validation_sample_rate = validation_sample_rate\n",
    "            else:\n",
    "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
    "        else:\n",
    "            self.use_validation_set = False\n",
    "            self.validation_sample_rate = 0\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_lr = minimum_lr\n",
    "        self.final_lr = maximum_lr\n",
    "        self.lr_scale = lr_scale\n",
    "        self.stopping_criterion_factor = stopping_criterion_factor\n",
    "        self.loss_smoothing_beta = loss_smoothing_beta\n",
    "        self.save_dir = save_dir\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_batches_ = num_samples // batch_size\n",
    "        self.current_lr_ = minimum_lr\n",
    "\n",
    "        if lr_scale == 'exp':\n",
    "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
    "                1. / float(self.num_batches_))\n",
    "        else:\n",
    "            extra_batch = int((num_samples % batch_size) != 0)\n",
    "            self.lr_multiplier_ = np.linspace(\n",
    "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
    "\n",
    "        # If negative, use entire validation set\n",
    "        if self.validation_sample_rate < 0:\n",
    "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
    "\n",
    "        self.current_batch_ = 0\n",
    "        self.current_epoch_ = 0\n",
    "        self.best_loss_ = 1e6\n",
    "        self.running_loss_ = 0.\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.current_epoch_ = 1\n",
    "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_batch_ = 0\n",
    "\n",
    "        if self.current_epoch_ > 1:\n",
    "            warnings.warn(\n",
    "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
    "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.current_batch_ += 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.current_epoch_ > 1:\n",
    "            return\n",
    "\n",
    "        if self.use_validation_set:\n",
    "            X, Y = self.validation_data[0], self.validation_data[1]\n",
    "\n",
    "            # use 5 random batches from test set for fast approximate of loss\n",
    "            num_samples = self.batch_size * self.validation_sample_rate\n",
    "\n",
    "            if num_samples > X.shape[0]:\n",
    "                num_samples = X.shape[0]\n",
    "\n",
    "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "            x = X[idx]\n",
    "            y = Y[idx]\n",
    "\n",
    "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
    "            loss = values[0]\n",
    "        else:\n",
    "            loss = logs['loss']\n",
    "\n",
    "        # smooth the loss value and bias correct\n",
    "        running_loss = self.loss_smoothing_beta * loss + (\n",
    "            1. - self.loss_smoothing_beta) * loss\n",
    "        running_loss = running_loss / (\n",
    "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
    "\n",
    "        # stop logging if loss is too large\n",
    "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
    "                running_loss >\n",
    "                self.stopping_criterion_factor * self.best_loss_):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
    "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
    "            return\n",
    "\n",
    "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
    "            self.best_loss_ = running_loss\n",
    "\n",
    "        current_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
    "        if self.lr_scale == 'exp':\n",
    "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
    "        else:\n",
    "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
    "\n",
    "        # compute the lr for the next batch and update the optimizer lr\n",
    "        if self.lr_scale == 'exp':\n",
    "            current_lr *= self.lr_multiplier_\n",
    "        else:\n",
    "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, current_lr)\n",
    "\n",
    "        # save the other metrics as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        if self.verbose:\n",
    "            if self.use_validation_set:\n",
    "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
    "                      (values[0], current_lr))\n",
    "            else:\n",
    "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
    "            if not os.path.exists(self.save_dir):\n",
    "                os.makedirs(self.save_dir)\n",
    "\n",
    "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
    "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
    "\n",
    "            np.save(losses_path, self.losses)\n",
    "            np.save(lrs_path, self.lrs)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
    "                      % (self.save_dir))\n",
    "\n",
    "        self.current_epoch_ += 1\n",
    "\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the callback itself.\n",
    "\n",
    "        # Arguments:\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses = self.losses\n",
    "        lrs = self.lrs\n",
    "\n",
    "        if clip_beginning:\n",
    "            losses = losses[clip_beginning:]\n",
    "            lrs = lrs[clip_beginning:]\n",
    "\n",
    "        if clip_endding:\n",
    "            losses = losses[:clip_endding]\n",
    "            lrs = lrs[:clip_endding]\n",
    "\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.title('Learning rate vs Loss')\n",
    "        plt.xlabel('learning rate')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def restore_schedule_from_dir(cls,\n",
    "                                  directory,\n",
    "                                  clip_beginning=None,\n",
    "                                  clip_endding=None):\n",
    "        \"\"\"\n",
    "        Loads the training history from the saved numpy files in the given directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "\n",
    "        Returns:\n",
    "            tuple of (losses, learning rates)\n",
    "        \"\"\"\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses_path = os.path.join(directory, 'losses.npy')\n",
    "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
    "\n",
    "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
    "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
    "                  (losses_path, lrs_path, directory))\n",
    "\n",
    "            losses = None\n",
    "            lrs = None\n",
    "\n",
    "        else:\n",
    "            losses = np.load(losses_path)\n",
    "            lrs = np.load(lrs_path)\n",
    "\n",
    "            if clip_beginning:\n",
    "                losses = losses[clip_beginning:]\n",
    "                lrs = lrs[clip_beginning:]\n",
    "\n",
    "            if clip_endding:\n",
    "                losses = losses[:clip_endding]\n",
    "                lrs = lrs[:clip_endding]\n",
    "\n",
    "        return losses, lrs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_schedule_from_file(cls,\n",
    "                                directory,\n",
    "                                clip_beginning=None,\n",
    "                                clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
    "        rate values in the specified directory.\n",
    "\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
    "            return\n",
    "\n",
    "        losses, lrs = cls.restore_schedule_from_dir(\n",
    "            directory,\n",
    "            clip_beginning=clip_beginning,\n",
    "            clip_endding=clip_endding)\n",
    "\n",
    "        if losses is None or lrs is None:\n",
    "            return\n",
    "        else:\n",
    "            plt.plot(lrs, losses)\n",
    "            plt.title('Learning rate vs Loss')\n",
    "            plt.xlabel('learning rate')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "    @property\n",
    "    def lrs(self):\n",
    "        return np.array(self.history['log_lrs'])\n",
    "\n",
    "    @property\n",
    "    def losses(self):\n",
    "        return np.array(self.history['running_loss_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28798 samples, validate on 9600 samples\n",
      "Epoch 1/50\n",
      " - 84s - loss: 11.0557 - mean_absolute_error: 2.5347 - val_loss: 7.3956 - val_mean_absolute_error: 2.1980\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 2.19802, saving model to lanl0.h5\n",
      "Epoch 2/50\n",
      " - 86s - loss: 8.9079 - mean_absolute_error: 2.2814 - val_loss: 8.7193 - val_mean_absolute_error: 2.3759\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error did not improve from 2.19802\n",
      "Epoch 3/50\n",
      " - 86s - loss: 8.6441 - mean_absolute_error: 2.2460 - val_loss: 7.2213 - val_mean_absolute_error: 2.0869\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 2.19802 to 2.08691, saving model to lanl0.h5\n",
      "Epoch 4/50\n",
      " - 86s - loss: 8.4690 - mean_absolute_error: 2.2243 - val_loss: 8.7469 - val_mean_absolute_error: 2.3419\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 5/50\n",
      " - 86s - loss: 8.3567 - mean_absolute_error: 2.2186 - val_loss: 12.8717 - val_mean_absolute_error: 2.9505\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 6/50\n",
      " - 86s - loss: 8.0800 - mean_absolute_error: 2.1801 - val_loss: 7.4670 - val_mean_absolute_error: 2.1533\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 7/50\n",
      " - 86s - loss: 7.9532 - mean_absolute_error: 2.1619 - val_loss: 12.6568 - val_mean_absolute_error: 2.9089\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 8/50\n",
      " - 86s - loss: 7.7540 - mean_absolute_error: 2.1341 - val_loss: 22.4585 - val_mean_absolute_error: 4.1712\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 9/50\n",
      " - 86s - loss: 7.6152 - mean_absolute_error: 2.1152 - val_loss: 16.3728 - val_mean_absolute_error: 3.3840\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 10/50\n",
      " - 86s - loss: 7.4620 - mean_absolute_error: 2.0952 - val_loss: 30.7964 - val_mean_absolute_error: 5.0609\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 11/50\n",
      " - 86s - loss: 7.3586 - mean_absolute_error: 2.0833 - val_loss: 12.5939 - val_mean_absolute_error: 2.8539\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 12/50\n",
      " - 86s - loss: 7.1925 - mean_absolute_error: 2.0599 - val_loss: 38.8502 - val_mean_absolute_error: 5.7599\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 13/50\n",
      " - 86s - loss: 7.1688 - mean_absolute_error: 2.0563 - val_loss: 44.0391 - val_mean_absolute_error: 6.1955\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 14/50\n",
      " - 86s - loss: 7.0930 - mean_absolute_error: 2.0495 - val_loss: 38.1972 - val_mean_absolute_error: 5.6886\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 15/50\n",
      " - 86s - loss: 7.0505 - mean_absolute_error: 2.0439 - val_loss: 25.5448 - val_mean_absolute_error: 4.4445\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 16/50\n",
      " - 86s - loss: 6.9276 - mean_absolute_error: 2.0252 - val_loss: 52.8414 - val_mean_absolute_error: 6.8269\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 17/50\n",
      " - 86s - loss: 6.9071 - mean_absolute_error: 2.0228 - val_loss: 33.9764 - val_mean_absolute_error: 5.2886\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "Epoch 18/50\n",
      " - 86s - loss: 6.8920 - mean_absolute_error: 2.0234 - val_loss: 39.7061 - val_mean_absolute_error: 5.7769\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 19/50\n",
      " - 87s - loss: 6.8879 - mean_absolute_error: 2.0162 - val_loss: 51.3096 - val_mean_absolute_error: 6.6990\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "Epoch 20/50\n",
      " - 86s - loss: 6.7974 - mean_absolute_error: 2.0096 - val_loss: 54.8701 - val_mean_absolute_error: 6.9576\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 21/50\n",
      " - 86s - loss: 6.7571 - mean_absolute_error: 2.0035 - val_loss: 39.3902 - val_mean_absolute_error: 5.7508\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
      "Epoch 22/50\n",
      " - 86s - loss: 6.7987 - mean_absolute_error: 2.0072 - val_loss: 47.1669 - val_mean_absolute_error: 6.3818\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 23/50\n",
      " - 86s - loss: 6.7592 - mean_absolute_error: 2.0072 - val_loss: 55.4102 - val_mean_absolute_error: 6.9702\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
      "Epoch 24/50\n",
      " - 86s - loss: 6.7056 - mean_absolute_error: 1.9953 - val_loss: 52.1251 - val_mean_absolute_error: 6.7373\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 25/50\n",
      " - 86s - loss: 6.7299 - mean_absolute_error: 2.0058 - val_loss: 58.6017 - val_mean_absolute_error: 7.2005\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
      "Epoch 26/50\n",
      " - 86s - loss: 6.6438 - mean_absolute_error: 1.9835 - val_loss: 43.5842 - val_mean_absolute_error: 6.0723\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 27/50\n",
      " - 86s - loss: 6.7306 - mean_absolute_error: 2.0039 - val_loss: 59.0564 - val_mean_absolute_error: 7.2364\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
      "Epoch 28/50\n",
      " - 86s - loss: 6.6636 - mean_absolute_error: 1.9869 - val_loss: 55.8420 - val_mean_absolute_error: 6.9963\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 29/50\n",
      " - 86s - loss: 6.7042 - mean_absolute_error: 1.9975 - val_loss: 56.8192 - val_mean_absolute_error: 7.0711\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 2.08691\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 30/50\n",
      " - 86s - loss: 6.6875 - mean_absolute_error: 1.9934 - val_loss: 53.8956 - val_mean_absolute_error: 6.8487\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 31/50\n",
      " - 86s - loss: 6.6636 - mean_absolute_error: 1.9922 - val_loss: 54.8384 - val_mean_absolute_error: 6.9288\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 32/50\n",
      " - 86s - loss: 6.6519 - mean_absolute_error: 1.9927 - val_loss: 57.7221 - val_mean_absolute_error: 7.1284\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 33/50\n",
      " - 86s - loss: 6.6768 - mean_absolute_error: 1.9912 - val_loss: 61.8608 - val_mean_absolute_error: 7.4043\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 34/50\n",
      " - 86s - loss: 6.6791 - mean_absolute_error: 1.9935 - val_loss: 49.4359 - val_mean_absolute_error: 6.5247\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 35/50\n",
      " - 86s - loss: 6.6703 - mean_absolute_error: 1.9936 - val_loss: 44.9604 - val_mean_absolute_error: 6.1706\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 36/50\n",
      " - 86s - loss: 6.6539 - mean_absolute_error: 1.9841 - val_loss: 49.7098 - val_mean_absolute_error: 6.5520\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 37/50\n",
      " - 86s - loss: 6.6050 - mean_absolute_error: 1.9838 - val_loss: 52.5897 - val_mean_absolute_error: 6.7569\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 38/50\n",
      " - 86s - loss: 6.6722 - mean_absolute_error: 1.9909 - val_loss: 59.3772 - val_mean_absolute_error: 7.2308\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 39/50\n",
      " - 86s - loss: 6.6199 - mean_absolute_error: 1.9833 - val_loss: 61.6573 - val_mean_absolute_error: 7.3965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 40/50\n",
      " - 86s - loss: 6.6550 - mean_absolute_error: 1.9888 - val_loss: 56.3826 - val_mean_absolute_error: 7.0243\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 41/50\n",
      " - 86s - loss: 6.6546 - mean_absolute_error: 1.9906 - val_loss: 46.3657 - val_mean_absolute_error: 6.2857\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 42/50\n",
      " - 86s - loss: 6.6863 - mean_absolute_error: 1.9954 - val_loss: 58.4183 - val_mean_absolute_error: 7.1609\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 43/50\n",
      " - 86s - loss: 6.6302 - mean_absolute_error: 1.9830 - val_loss: 49.7170 - val_mean_absolute_error: 6.5275\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 44/50\n",
      " - 86s - loss: 6.6820 - mean_absolute_error: 1.9953 - val_loss: 49.1644 - val_mean_absolute_error: 6.4885\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 45/50\n",
      " - 86s - loss: 6.6332 - mean_absolute_error: 1.9858 - val_loss: 55.1551 - val_mean_absolute_error: 6.9499\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 46/50\n",
      " - 86s - loss: 6.6223 - mean_absolute_error: 1.9845 - val_loss: 48.1185 - val_mean_absolute_error: 6.3946\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 47/50\n",
      " - 86s - loss: 6.6089 - mean_absolute_error: 1.9833 - val_loss: 59.8081 - val_mean_absolute_error: 7.2572\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 48/50\n",
      " - 86s - loss: 6.6421 - mean_absolute_error: 1.9887 - val_loss: 54.9941 - val_mean_absolute_error: 6.9297\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 49/50\n",
      " - 86s - loss: 6.6329 - mean_absolute_error: 1.9884 - val_loss: 61.0028 - val_mean_absolute_error: 7.3451\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 2.08691\n",
      "Epoch 50/50\n",
      " - 86s - loss: 6.5898 - mean_absolute_error: 1.9806 - val_loss: 58.6078 - val_mean_absolute_error: 7.1805\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 2.08691\n",
      "Train on 28798 samples, validate on 9600 samples\n",
      "Epoch 1/50\n",
      " - 87s - loss: 9.9349 - mean_absolute_error: 2.4329 - val_loss: 11.8015 - val_mean_absolute_error: 2.8494\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 2.84936, saving model to lanl1.h5\n",
      "Epoch 2/50\n",
      " - 86s - loss: 7.5660 - mean_absolute_error: 2.1152 - val_loss: 11.5418 - val_mean_absolute_error: 2.8091\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 2.84936 to 2.80913, saving model to lanl1.h5\n",
      "Epoch 3/50\n",
      " - 86s - loss: 7.2947 - mean_absolute_error: 2.0856 - val_loss: 35.8343 - val_mean_absolute_error: 5.0674\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error did not improve from 2.80913\n",
      "Epoch 4/50\n",
      " - 86s - loss: 7.1456 - mean_absolute_error: 2.0709 - val_loss: 12.0870 - val_mean_absolute_error: 2.7355\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 2.80913 to 2.73550, saving model to lanl1.h5\n",
      "Epoch 5/50\n",
      " - 86s - loss: 7.0580 - mean_absolute_error: 2.0526 - val_loss: 12.4988 - val_mean_absolute_error: 2.6829\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 2.73550 to 2.68292, saving model to lanl1.h5\n",
      "Epoch 6/50\n",
      " - 86s - loss: 6.8537 - mean_absolute_error: 2.0286 - val_loss: 15.6602 - val_mean_absolute_error: 2.8751\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error did not improve from 2.68292\n",
      "Epoch 7/50\n",
      " - 86s - loss: 6.8442 - mean_absolute_error: 2.0294 - val_loss: 11.7741 - val_mean_absolute_error: 2.6421\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error improved from 2.68292 to 2.64213, saving model to lanl1.h5\n",
      "Epoch 8/50\n",
      " - 86s - loss: 6.6905 - mean_absolute_error: 2.0003 - val_loss: 24.2857 - val_mean_absolute_error: 3.8623\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 9/50\n",
      " - 86s - loss: 6.6381 - mean_absolute_error: 1.9963 - val_loss: 23.9968 - val_mean_absolute_error: 3.8387\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 10/50\n",
      " - 86s - loss: 6.4326 - mean_absolute_error: 1.9661 - val_loss: 13.9836 - val_mean_absolute_error: 2.7462\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 11/50\n",
      " - 86s - loss: 6.3978 - mean_absolute_error: 1.9613 - val_loss: 18.2563 - val_mean_absolute_error: 3.2421\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 12/50\n",
      " - 85s - loss: 6.1848 - mean_absolute_error: 1.9256 - val_loss: 42.1927 - val_mean_absolute_error: 5.6273\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 13/50\n",
      " - 85s - loss: 6.0973 - mean_absolute_error: 1.9130 - val_loss: 17.2583 - val_mean_absolute_error: 3.0353\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 14/50\n",
      " - 85s - loss: 5.9383 - mean_absolute_error: 1.8883 - val_loss: 21.6557 - val_mean_absolute_error: 3.5453\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 15/50\n",
      " - 85s - loss: 5.8856 - mean_absolute_error: 1.8811 - val_loss: 36.9156 - val_mean_absolute_error: 5.1621\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 16/50\n",
      " - 85s - loss: 5.8137 - mean_absolute_error: 1.8662 - val_loss: 31.3018 - val_mean_absolute_error: 4.6134\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 17/50\n",
      " - 85s - loss: 5.7605 - mean_absolute_error: 1.8609 - val_loss: 26.3489 - val_mean_absolute_error: 4.0847\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 18/50\n",
      " - 85s - loss: 5.6334 - mean_absolute_error: 1.8379 - val_loss: 33.4088 - val_mean_absolute_error: 4.8353\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 19/50\n",
      " - 85s - loss: 5.6286 - mean_absolute_error: 1.8424 - val_loss: 46.5690 - val_mean_absolute_error: 6.0041\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 20/50\n",
      " - 85s - loss: 5.5894 - mean_absolute_error: 1.8367 - val_loss: 22.2236 - val_mean_absolute_error: 3.5972\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 21/50\n",
      " - 85s - loss: 5.5331 - mean_absolute_error: 1.8277 - val_loss: 30.1086 - val_mean_absolute_error: 4.4793\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "Epoch 22/50\n",
      " - 85s - loss: 5.4796 - mean_absolute_error: 1.8158 - val_loss: 37.9700 - val_mean_absolute_error: 5.2539\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 23/50\n",
      " - 85s - loss: 5.4407 - mean_absolute_error: 1.8155 - val_loss: 54.4197 - val_mean_absolute_error: 6.6092\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "Epoch 24/50\n",
      " - 85s - loss: 5.4128 - mean_absolute_error: 1.8072 - val_loss: 40.0439 - val_mean_absolute_error: 5.4234\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 25/50\n",
      " - 85s - loss: 5.4253 - mean_absolute_error: 1.8077 - val_loss: 42.5385 - val_mean_absolute_error: 5.6445\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
      "Epoch 26/50\n",
      " - 85s - loss: 5.4535 - mean_absolute_error: 1.8103 - val_loss: 34.3542 - val_mean_absolute_error: 4.8706\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 27/50\n",
      " - 85s - loss: 5.3329 - mean_absolute_error: 1.7918 - val_loss: 45.1040 - val_mean_absolute_error: 5.8639\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
      "Epoch 28/50\n",
      " - 85s - loss: 5.3804 - mean_absolute_error: 1.8055 - val_loss: 53.5311 - val_mean_absolute_error: 6.5337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 29/50\n",
      " - 85s - loss: 5.3562 - mean_absolute_error: 1.7957 - val_loss: 41.5186 - val_mean_absolute_error: 5.5496\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
      "Epoch 30/50\n",
      " - 85s - loss: 5.3743 - mean_absolute_error: 1.8015 - val_loss: 54.5762 - val_mean_absolute_error: 6.5963\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 31/50\n",
      " - 85s - loss: 5.3602 - mean_absolute_error: 1.8005 - val_loss: 39.0258 - val_mean_absolute_error: 5.3083\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
      "Epoch 32/50\n",
      " - 85s - loss: 5.3615 - mean_absolute_error: 1.7935 - val_loss: 55.8593 - val_mean_absolute_error: 6.6896\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 33/50\n",
      " - 85s - loss: 5.3133 - mean_absolute_error: 1.7844 - val_loss: 61.1484 - val_mean_absolute_error: 7.0808\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 2.64213\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 34/50\n",
      " - 85s - loss: 5.3443 - mean_absolute_error: 1.7922 - val_loss: 52.0596 - val_mean_absolute_error: 6.4033\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 35/50\n",
      " - 85s - loss: 5.3632 - mean_absolute_error: 1.7959 - val_loss: 38.5560 - val_mean_absolute_error: 5.2597\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 36/50\n",
      " - 85s - loss: 5.3195 - mean_absolute_error: 1.7915 - val_loss: 43.0014 - val_mean_absolute_error: 5.6610\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 37/50\n",
      " - 85s - loss: 5.3016 - mean_absolute_error: 1.7894 - val_loss: 55.4091 - val_mean_absolute_error: 6.6564\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 38/50\n",
      " - 85s - loss: 5.3299 - mean_absolute_error: 1.7941 - val_loss: 49.9028 - val_mean_absolute_error: 6.2284\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 39/50\n",
      " - 85s - loss: 5.2940 - mean_absolute_error: 1.7862 - val_loss: 43.8886 - val_mean_absolute_error: 5.7343\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 40/50\n",
      " - 85s - loss: 5.2825 - mean_absolute_error: 1.7830 - val_loss: 47.1785 - val_mean_absolute_error: 6.0041\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 41/50\n",
      " - 85s - loss: 5.3049 - mean_absolute_error: 1.7913 - val_loss: 40.1911 - val_mean_absolute_error: 5.4047\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 42/50\n",
      " - 85s - loss: 5.3220 - mean_absolute_error: 1.7927 - val_loss: 49.8576 - val_mean_absolute_error: 6.2286\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 43/50\n",
      " - 85s - loss: 5.3156 - mean_absolute_error: 1.7898 - val_loss: 58.7085 - val_mean_absolute_error: 6.9037\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 44/50\n",
      " - 85s - loss: 5.3040 - mean_absolute_error: 1.7904 - val_loss: 51.9418 - val_mean_absolute_error: 6.3897\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 45/50\n",
      " - 85s - loss: 5.3082 - mean_absolute_error: 1.7888 - val_loss: 43.0305 - val_mean_absolute_error: 5.6613\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 46/50\n",
      " - 85s - loss: 5.3122 - mean_absolute_error: 1.7930 - val_loss: 43.9276 - val_mean_absolute_error: 5.7218\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 47/50\n",
      " - 85s - loss: 5.3215 - mean_absolute_error: 1.7908 - val_loss: 49.1678 - val_mean_absolute_error: 6.1669\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 48/50\n",
      " - 85s - loss: 5.2599 - mean_absolute_error: 1.7828 - val_loss: 36.2976 - val_mean_absolute_error: 5.0225\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 49/50\n",
      " - 85s - loss: 5.3199 - mean_absolute_error: 1.7896 - val_loss: 38.9294 - val_mean_absolute_error: 5.2712\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 2.64213\n",
      "Epoch 50/50\n",
      " - 85s - loss: 5.3240 - mean_absolute_error: 1.7885 - val_loss: 48.9185 - val_mean_absolute_error: 6.1509\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 2.64213\n",
      "Train on 28799 samples, validate on 9599 samples\n",
      "Epoch 1/50\n",
      " - 86s - loss: 12.4968 - mean_absolute_error: 2.7847 - val_loss: 11.9542 - val_mean_absolute_error: 2.8337\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 2.83366, saving model to lanl2.h5\n",
      "Epoch 2/50\n",
      " - 85s - loss: 9.9647 - mean_absolute_error: 2.5170 - val_loss: 4.3378 - val_mean_absolute_error: 1.5550\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 2.83366 to 1.55500, saving model to lanl2.h5\n",
      "Epoch 3/50\n",
      " - 85s - loss: 9.7002 - mean_absolute_error: 2.4841 - val_loss: 4.1192 - val_mean_absolute_error: 1.5254\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error improved from 1.55500 to 1.52539, saving model to lanl2.h5\n",
      "Epoch 4/50\n",
      " - 85s - loss: 9.5309 - mean_absolute_error: 2.4636 - val_loss: 16.4448 - val_mean_absolute_error: 3.5457\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error did not improve from 1.52539\n",
      "Epoch 5/50\n",
      " - 85s - loss: 9.3749 - mean_absolute_error: 2.4457 - val_loss: 4.0383 - val_mean_absolute_error: 1.5159\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error improved from 1.52539 to 1.51592, saving model to lanl2.h5\n",
      "Epoch 6/50\n",
      " - 85s - loss: 9.2482 - mean_absolute_error: 2.4280 - val_loss: 11.6289 - val_mean_absolute_error: 2.9428\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error did not improve from 1.51592\n",
      "Epoch 7/50\n",
      " - 85s - loss: 9.1285 - mean_absolute_error: 2.4159 - val_loss: 8.0685 - val_mean_absolute_error: 2.3992\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 1.51592\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 8/50\n",
      " - 85s - loss: 8.8821 - mean_absolute_error: 2.3895 - val_loss: 3.8604 - val_mean_absolute_error: 1.4892\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error improved from 1.51592 to 1.48918, saving model to lanl2.h5\n",
      "Epoch 9/50\n",
      " - 85s - loss: 8.7748 - mean_absolute_error: 2.3707 - val_loss: 5.7867 - val_mean_absolute_error: 1.8304\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 10/50\n",
      " - 85s - loss: 8.6526 - mean_absolute_error: 2.3529 - val_loss: 4.0603 - val_mean_absolute_error: 1.5789\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 11/50\n",
      " - 85s - loss: 8.4471 - mean_absolute_error: 2.3215 - val_loss: 21.9975 - val_mean_absolute_error: 4.3718\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 12/50\n",
      " - 85s - loss: 8.3330 - mean_absolute_error: 2.3011 - val_loss: 24.1962 - val_mean_absolute_error: 4.5915\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 13/50\n",
      " - 85s - loss: 8.0868 - mean_absolute_error: 2.2721 - val_loss: 20.2893 - val_mean_absolute_error: 4.1689\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 14/50\n",
      " - 85s - loss: 8.0234 - mean_absolute_error: 2.2576 - val_loss: 5.4296 - val_mean_absolute_error: 1.8678\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 15/50\n",
      " - 85s - loss: 7.9231 - mean_absolute_error: 2.2438 - val_loss: 8.6467 - val_mean_absolute_error: 2.4694\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 16/50\n",
      " - 85s - loss: 7.8121 - mean_absolute_error: 2.2312 - val_loss: 34.6957 - val_mean_absolute_error: 5.5913\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 17/50\n",
      " - 85s - loss: 7.7175 - mean_absolute_error: 2.2133 - val_loss: 16.7482 - val_mean_absolute_error: 3.6878\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 18/50\n",
      " - 85s - loss: 7.5920 - mean_absolute_error: 2.1959 - val_loss: 16.5180 - val_mean_absolute_error: 3.6060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 19/50\n",
      " - 85s - loss: 7.5823 - mean_absolute_error: 2.1905 - val_loss: 21.7690 - val_mean_absolute_error: 4.2628\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 20/50\n",
      " - 85s - loss: 7.5520 - mean_absolute_error: 2.1892 - val_loss: 12.4380 - val_mean_absolute_error: 3.0577\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "Epoch 21/50\n",
      " - 85s - loss: 7.4970 - mean_absolute_error: 2.1824 - val_loss: 23.6670 - val_mean_absolute_error: 4.4815\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 22/50\n",
      " - 85s - loss: 7.4981 - mean_absolute_error: 2.1813 - val_loss: 20.1302 - val_mean_absolute_error: 4.0528\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "Epoch 23/50\n",
      " - 85s - loss: 7.4451 - mean_absolute_error: 2.1718 - val_loss: 18.9627 - val_mean_absolute_error: 3.9420\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 24/50\n",
      " - 85s - loss: 7.4552 - mean_absolute_error: 2.1752 - val_loss: 11.5855 - val_mean_absolute_error: 2.8748\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
      "Epoch 25/50\n",
      " - 85s - loss: 7.3580 - mean_absolute_error: 2.1554 - val_loss: 21.8238 - val_mean_absolute_error: 4.2443\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 26/50\n",
      " - 85s - loss: 7.3726 - mean_absolute_error: 2.1598 - val_loss: 17.5261 - val_mean_absolute_error: 3.7107\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
      "Epoch 27/50\n",
      " - 85s - loss: 7.3510 - mean_absolute_error: 2.1555 - val_loss: 21.8715 - val_mean_absolute_error: 4.2493\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 28/50\n",
      " - 85s - loss: 7.3294 - mean_absolute_error: 2.1567 - val_loss: 20.2348 - val_mean_absolute_error: 4.0458\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
      "Epoch 29/50\n",
      " - 85s - loss: 7.3360 - mean_absolute_error: 2.1575 - val_loss: 23.2873 - val_mean_absolute_error: 4.3816\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 30/50\n",
      " - 85s - loss: 7.3095 - mean_absolute_error: 2.1521 - val_loss: 23.6604 - val_mean_absolute_error: 4.4620\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
      "Epoch 31/50\n",
      " - 85s - loss: 7.2914 - mean_absolute_error: 2.1525 - val_loss: 27.6897 - val_mean_absolute_error: 4.8787\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 32/50\n",
      " - 85s - loss: 7.2588 - mean_absolute_error: 2.1492 - val_loss: 24.3772 - val_mean_absolute_error: 4.5130\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 1.48918\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 33/50\n",
      " - 85s - loss: 7.2749 - mean_absolute_error: 2.1459 - val_loss: 20.8966 - val_mean_absolute_error: 4.1007\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 34/50\n",
      " - 85s - loss: 7.3095 - mean_absolute_error: 2.1535 - val_loss: 17.8297 - val_mean_absolute_error: 3.7081\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 35/50\n",
      " - 85s - loss: 7.3066 - mean_absolute_error: 2.1562 - val_loss: 13.8127 - val_mean_absolute_error: 3.1274\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 36/50\n",
      " - 85s - loss: 7.2513 - mean_absolute_error: 2.1456 - val_loss: 20.6903 - val_mean_absolute_error: 4.0634\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 37/50\n",
      " - 85s - loss: 7.2295 - mean_absolute_error: 2.1427 - val_loss: 22.7972 - val_mean_absolute_error: 4.3213\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 38/50\n",
      " - 85s - loss: 7.2653 - mean_absolute_error: 2.1433 - val_loss: 22.1913 - val_mean_absolute_error: 4.2675\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 39/50\n",
      " - 85s - loss: 7.2679 - mean_absolute_error: 2.1442 - val_loss: 17.8965 - val_mean_absolute_error: 3.6950\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 40/50\n",
      " - 85s - loss: 7.2747 - mean_absolute_error: 2.1477 - val_loss: 21.9521 - val_mean_absolute_error: 4.2195\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 41/50\n",
      " - 85s - loss: 7.2836 - mean_absolute_error: 2.1502 - val_loss: 20.1598 - val_mean_absolute_error: 3.9743\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 42/50\n",
      " - 85s - loss: 7.2372 - mean_absolute_error: 2.1461 - val_loss: 23.7433 - val_mean_absolute_error: 4.4330\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 43/50\n",
      " - 85s - loss: 7.2486 - mean_absolute_error: 2.1445 - val_loss: 16.3524 - val_mean_absolute_error: 3.4830\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 44/50\n",
      " - 85s - loss: 7.1982 - mean_absolute_error: 2.1366 - val_loss: 22.7318 - val_mean_absolute_error: 4.3082\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 45/50\n",
      " - 85s - loss: 7.2175 - mean_absolute_error: 2.1412 - val_loss: 19.1312 - val_mean_absolute_error: 3.8477\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 46/50\n",
      " - 85s - loss: 7.2822 - mean_absolute_error: 2.1502 - val_loss: 25.5485 - val_mean_absolute_error: 4.6093\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 47/50\n",
      " - 86s - loss: 7.2067 - mean_absolute_error: 2.1384 - val_loss: 20.5544 - val_mean_absolute_error: 4.0296\n",
      "\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 48/50\n",
      " - 86s - loss: 7.2903 - mean_absolute_error: 2.1508 - val_loss: 21.9466 - val_mean_absolute_error: 4.2058\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 49/50\n",
      " - 86s - loss: 7.2572 - mean_absolute_error: 2.1476 - val_loss: 22.1261 - val_mean_absolute_error: 4.2422\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 1.48918\n",
      "Epoch 50/50\n",
      " - 86s - loss: 7.2653 - mean_absolute_error: 2.1458 - val_loss: 23.6341 - val_mean_absolute_error: 4.4062\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 1.48918\n",
      "Train on 28799 samples, validate on 9599 samples\n",
      "Epoch 1/50\n",
      " - 87s - loss: 10.6906 - mean_absolute_error: 2.5056 - val_loss: 8.9851 - val_mean_absolute_error: 2.3642\n",
      "\n",
      "Epoch 00001: val_mean_absolute_error improved from inf to 2.36420, saving model to lanl3.h5\n",
      "Epoch 2/50\n",
      " - 86s - loss: 8.5808 - mean_absolute_error: 2.2578 - val_loss: 8.7195 - val_mean_absolute_error: 2.3514\n",
      "\n",
      "Epoch 00002: val_mean_absolute_error improved from 2.36420 to 2.35143, saving model to lanl3.h5\n",
      "Epoch 3/50\n",
      " - 86s - loss: 8.2452 - mean_absolute_error: 2.2179 - val_loss: 9.2761 - val_mean_absolute_error: 2.5275\n",
      "\n",
      "Epoch 00003: val_mean_absolute_error did not improve from 2.35143\n",
      "Epoch 4/50\n",
      " - 86s - loss: 8.1390 - mean_absolute_error: 2.2069 - val_loss: 8.6337 - val_mean_absolute_error: 2.2537\n",
      "\n",
      "Epoch 00004: val_mean_absolute_error improved from 2.35143 to 2.25374, saving model to lanl3.h5\n",
      "Epoch 5/50\n",
      " - 86s - loss: 7.9875 - mean_absolute_error: 2.1895 - val_loss: 12.3460 - val_mean_absolute_error: 2.5746\n",
      "\n",
      "Epoch 00005: val_mean_absolute_error did not improve from 2.25374\n",
      "Epoch 6/50\n",
      " - 85s - loss: 7.8957 - mean_absolute_error: 2.1781 - val_loss: 8.8055 - val_mean_absolute_error: 2.2153\n",
      "\n",
      "Epoch 00006: val_mean_absolute_error improved from 2.25374 to 2.21529, saving model to lanl3.h5\n",
      "Epoch 7/50\n",
      " - 85s - loss: 7.7987 - mean_absolute_error: 2.1649 - val_loss: 14.8115 - val_mean_absolute_error: 2.9086\n",
      "\n",
      "Epoch 00007: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 8/50\n",
      " - 85s - loss: 7.7287 - mean_absolute_error: 2.1545 - val_loss: 10.3445 - val_mean_absolute_error: 2.3138\n",
      "\n",
      "Epoch 00008: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 9/50\n",
      " - 85s - loss: 7.4560 - mean_absolute_error: 2.1153 - val_loss: 12.3555 - val_mean_absolute_error: 2.5636\n",
      "\n",
      "Epoch 00009: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 10/50\n",
      " - 85s - loss: 7.3378 - mean_absolute_error: 2.0993 - val_loss: 11.3252 - val_mean_absolute_error: 2.4393\n",
      "\n",
      "Epoch 00010: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 11/50\n",
      " - 85s - loss: 7.1546 - mean_absolute_error: 2.0749 - val_loss: 37.7471 - val_mean_absolute_error: 5.5185\n",
      "\n",
      "Epoch 00011: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 12/50\n",
      " - 85s - loss: 7.0258 - mean_absolute_error: 2.0536 - val_loss: 28.2004 - val_mean_absolute_error: 4.5847\n",
      "\n",
      "Epoch 00012: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 13/50\n",
      " - 85s - loss: 6.9110 - mean_absolute_error: 2.0377 - val_loss: 19.6461 - val_mean_absolute_error: 3.5671\n",
      "\n",
      "Epoch 00013: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 14/50\n",
      " - 85s - loss: 6.8661 - mean_absolute_error: 2.0300 - val_loss: 41.5400 - val_mean_absolute_error: 5.8315\n",
      "\n",
      "Epoch 00014: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 15/50\n",
      " - 85s - loss: 6.7372 - mean_absolute_error: 2.0128 - val_loss: 57.9984 - val_mean_absolute_error: 7.0666\n",
      "\n",
      "Epoch 00015: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 16/50\n",
      " - 85s - loss: 6.6461 - mean_absolute_error: 1.9992 - val_loss: 56.7307 - val_mean_absolute_error: 6.9685\n",
      "\n",
      "Epoch 00016: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 17/50\n",
      " - 85s - loss: 6.5782 - mean_absolute_error: 1.9924 - val_loss: 51.6906 - val_mean_absolute_error: 6.6103\n",
      "\n",
      "Epoch 00017: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 18/50\n",
      " - 85s - loss: 6.5830 - mean_absolute_error: 1.9912 - val_loss: 37.6043 - val_mean_absolute_error: 5.4499\n",
      "\n",
      "Epoch 00018: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 19/50\n",
      " - 85s - loss: 6.4592 - mean_absolute_error: 1.9699 - val_loss: 62.1038 - val_mean_absolute_error: 7.3312\n",
      "\n",
      "Epoch 00019: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 20/50\n",
      " - 85s - loss: 6.3791 - mean_absolute_error: 1.9603 - val_loss: 44.7734 - val_mean_absolute_error: 6.0601\n",
      "\n",
      "Epoch 00020: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "Epoch 21/50\n",
      " - 85s - loss: 6.3625 - mean_absolute_error: 1.9576 - val_loss: 61.8829 - val_mean_absolute_error: 7.3069\n",
      "\n",
      "Epoch 00021: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 22/50\n",
      " - 85s - loss: 6.3288 - mean_absolute_error: 1.9529 - val_loss: 48.4431 - val_mean_absolute_error: 6.3444\n",
      "\n",
      "Epoch 00022: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "Epoch 23/50\n",
      " - 85s - loss: 6.3050 - mean_absolute_error: 1.9492 - val_loss: 57.1329 - val_mean_absolute_error: 6.9747\n",
      "\n",
      "Epoch 00023: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 24/50\n",
      " - 85s - loss: 6.2729 - mean_absolute_error: 1.9410 - val_loss: 57.2295 - val_mean_absolute_error: 6.9877\n",
      "\n",
      "Epoch 00024: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
      "Epoch 25/50\n",
      " - 85s - loss: 6.3221 - mean_absolute_error: 1.9496 - val_loss: 64.3415 - val_mean_absolute_error: 7.4654\n",
      "\n",
      "Epoch 00025: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 26/50\n",
      " - 85s - loss: 6.1952 - mean_absolute_error: 1.9311 - val_loss: 70.5466 - val_mean_absolute_error: 7.8652\n",
      "\n",
      "Epoch 00026: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
      "Epoch 27/50\n",
      " - 85s - loss: 6.2289 - mean_absolute_error: 1.9363 - val_loss: 56.3043 - val_mean_absolute_error: 6.9144\n",
      "\n",
      "Epoch 00027: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 28/50\n",
      " - 85s - loss: 6.2460 - mean_absolute_error: 1.9404 - val_loss: 88.5587 - val_mean_absolute_error: 8.9213\n",
      "\n",
      "Epoch 00028: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
      "Epoch 29/50\n",
      " - 85s - loss: 6.2230 - mean_absolute_error: 1.9362 - val_loss: 71.2450 - val_mean_absolute_error: 7.9085\n",
      "\n",
      "Epoch 00029: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 30/50\n",
      " - 85s - loss: 6.2059 - mean_absolute_error: 1.9322 - val_loss: 70.4674 - val_mean_absolute_error: 7.8564\n",
      "\n",
      "Epoch 00030: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
      "Epoch 31/50\n",
      " - 86s - loss: 6.2020 - mean_absolute_error: 1.9360 - val_loss: 73.7000 - val_mean_absolute_error: 8.0545\n",
      "\n",
      "Epoch 00031: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 32/50\n",
      " - 85s - loss: 6.1899 - mean_absolute_error: 1.9323 - val_loss: 52.7445 - val_mean_absolute_error: 6.6520\n",
      "\n",
      "Epoch 00032: val_mean_absolute_error did not improve from 2.21529\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 33/50\n",
      " - 85s - loss: 6.2277 - mean_absolute_error: 1.9381 - val_loss: 84.3715 - val_mean_absolute_error: 8.6871\n",
      "\n",
      "Epoch 00033: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 34/50\n",
      " - 85s - loss: 6.1592 - mean_absolute_error: 1.9300 - val_loss: 74.0922 - val_mean_absolute_error: 8.0768\n",
      "\n",
      "Epoch 00034: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 35/50\n",
      " - 85s - loss: 6.1718 - mean_absolute_error: 1.9291 - val_loss: 69.8524 - val_mean_absolute_error: 7.8151\n",
      "\n",
      "Epoch 00035: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 36/50\n",
      " - 85s - loss: 6.1453 - mean_absolute_error: 1.9268 - val_loss: 73.5228 - val_mean_absolute_error: 8.0438\n",
      "\n",
      "Epoch 00036: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 37/50\n",
      " - 85s - loss: 6.2155 - mean_absolute_error: 1.9395 - val_loss: 64.5774 - val_mean_absolute_error: 7.4705\n",
      "\n",
      "Epoch 00037: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 38/50\n",
      " - 85s - loss: 6.1788 - mean_absolute_error: 1.9312 - val_loss: 64.7087 - val_mean_absolute_error: 7.4754\n",
      "\n",
      "Epoch 00038: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 39/50\n",
      " - 85s - loss: 6.2000 - mean_absolute_error: 1.9357 - val_loss: 87.8445 - val_mean_absolute_error: 8.8822\n",
      "\n",
      "Epoch 00039: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 40/50\n",
      " - 86s - loss: 6.1793 - mean_absolute_error: 1.9311 - val_loss: 66.4122 - val_mean_absolute_error: 7.5923\n",
      "\n",
      "Epoch 00040: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 41/50\n",
      " - 86s - loss: 6.1176 - mean_absolute_error: 1.9183 - val_loss: 57.3095 - val_mean_absolute_error: 6.9764\n",
      "\n",
      "Epoch 00041: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 42/50\n",
      " - 86s - loss: 6.2126 - mean_absolute_error: 1.9367 - val_loss: 69.8084 - val_mean_absolute_error: 7.8104\n",
      "\n",
      "Epoch 00042: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 43/50\n",
      " - 86s - loss: 6.1947 - mean_absolute_error: 1.9321 - val_loss: 72.6226 - val_mean_absolute_error: 7.9795\n",
      "\n",
      "Epoch 00043: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 44/50\n",
      " - 85s - loss: 6.1361 - mean_absolute_error: 1.9225 - val_loss: 56.7109 - val_mean_absolute_error: 6.9353\n",
      "\n",
      "Epoch 00044: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 45/50\n",
      " - 85s - loss: 6.1442 - mean_absolute_error: 1.9257 - val_loss: 62.6579 - val_mean_absolute_error: 7.3426\n",
      "\n",
      "Epoch 00045: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 46/50\n",
      " - 85s - loss: 6.1550 - mean_absolute_error: 1.9241 - val_loss: 68.4098 - val_mean_absolute_error: 7.7208\n",
      "\n",
      "Epoch 00046: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 47/50\n",
      " - 85s - loss: 6.1597 - mean_absolute_error: 1.9278 - val_loss: 61.1372 - val_mean_absolute_error: 7.2370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 48/50\n",
      " - 85s - loss: 6.1353 - mean_absolute_error: 1.9240 - val_loss: 63.9680 - val_mean_absolute_error: 7.4304\n",
      "\n",
      "Epoch 00048: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 49/50\n",
      " - 85s - loss: 6.1016 - mean_absolute_error: 1.9197 - val_loss: 58.3632 - val_mean_absolute_error: 7.0469\n",
      "\n",
      "Epoch 00049: val_mean_absolute_error did not improve from 2.21529\n",
      "Epoch 50/50\n",
      " - 85s - loss: 6.1662 - mean_absolute_error: 1.9260 - val_loss: 67.5741 - val_mean_absolute_error: 7.6616\n",
      "\n",
      "Epoch 00050: val_mean_absolute_error did not improve from 2.21529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=4)\n",
    "i=0\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model =[]\n",
    "    for train_index, test_index in kf.split(img):\n",
    "        indep_train, indep_val = img[train_index], img[test_index]\n",
    "        dep_train, dep_val = dep[train_index], dep[test_index]\n",
    "        K.clear_session()\n",
    "        inp = Input((7,5000))\n",
    "        \n",
    "        x1 = Conv1D(128, 3, activation='selu', data_format='channels_first')(inp)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = MaxPooling1D(2, data_format='channels_first')(x1)\n",
    "        x1 = Conv1D(128, 3, activation='selu', data_format='channels_first')(x1)\n",
    "        x1 = Conv1D(128, 3, activation='selu', data_format='channels_first')(x1)\n",
    "        x = GlobalAveragePooling1D()(x1)\n",
    "        \n",
    "        x = Dropout(0.4)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        \n",
    "        gs1 = Model(inp, x)\n",
    "        gs1.compile(Adam(0.001), loss='mean_squared_error', metrics = ['mae'])\n",
    "        \n",
    "        clr_triangular = CyclicLR(base_lr=5e-6, max_lr=0.005, mode=\"triangular2\", step_size=2000)\n",
    "        \n",
    "        gs1.fit(indep_train, dep_train, validation_data=(indep_val, dep_val), batch_size=32, epochs=50, \n",
    "                callbacks = \n",
    "                          [\n",
    "                              ModelCheckpoint(\"lanl{0}.h5\".format(i), monitor='val_mean_absolute_error', verbose=1, save_best_only=True, mode='min'),\n",
    "                              ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.7, patience=2, \n",
    "                             verbose=1, mode='min', min_lr=0.00001),\n",
    "                       #       clr_triangular\n",
    "                          ],\n",
    "                verbose=2)\n",
    "        model.append(gs1)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"lanl.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(gs1.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAAJOCAYAAAAzj1duAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16108ae8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharex='col', figsize=(20, 10))\n",
    "ax.set_title('Model error history')\n",
    "ax.plot(gs1.history.history['mean_absolute_error'])\n",
    "ax.plot(gs1.history.history['val_mean_absolute_error'])\n",
    "ax.set_ylabel('Error')\n",
    "ax.legend(['train', 'test'], loc='right')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.041165560576157\n"
     ]
    }
   ],
   "source": [
    "indep_train, indep_val, dep_train, dep_val = train_test_split(img, dep, test_size=0.30)\n",
    "\n",
    "val = pd.DataFrame()\n",
    "for i in range(0,4):\n",
    "    with open(r\"lanl.yaml\".format(i), \"r\") as yaml_file:\n",
    "        K.clear_session()\n",
    "        m = model_from_yaml(yaml_file.read())\n",
    "        m.load_weights(r\"lanl{0}.h5\".format(i))\n",
    "        a = m.predict(indep_val)\n",
    "        val[\"model_{0}\".format(i)] = pd.Series(a.reshape(len(a)))\n",
    "        \n",
    "print(mean_absolute_error(val.iloc[:,:].mean(axis=1), dep_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2624/2624 [13:51<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "submission = pd.read_csv('sample_submission.csv', index_col = None)\n",
    "test = np.empty([0,7,5000])\n",
    "with tqdm.tqdm(total=submission.shape[0]) as bar:\n",
    "    for seg_id in submission[\"seg_id\"]:\n",
    "        single_sample = pd.read_csv('c:/users/ajaln/test/'+seg_id + '.csv')\n",
    "        w1 = pywt.cwt(single_sample[\"acoustic_data\"], [1, 5, 10, 15, 25, 50, 100], \"mexh\")[0][:,::30]\n",
    "        test = np.append(test, w1.reshape(1,7,5000), axis=0)\n",
    "        bar.update(1)\n",
    "np.save(\"res.npy\", test)\n",
    "#\"\"\";\n",
    "\n",
    "#test = np.load(\"res.npy\")\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in range(0, 4):\n",
    "    with open(r\"lanl.yaml\".format(i), \"r\") as yaml_file:\n",
    "        K.clear_session()\n",
    "        m = model_from_yaml(yaml_file.read())\n",
    "        m.load_weights(r\"lanl{0}.h5\".format(i))\n",
    "        a = m.predict(test)\n",
    "        data[\"model_{0}\".format(i)] = pd.Series(a.reshape(len(a)))\n",
    "        \n",
    "submission[\"time_to_failure\"] = data.iloc[:,:].mean(axis=1)\n",
    "submission.to_csv(r\"c:/work/dataset/earthquake/cnn_8folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209841"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 7, 5000)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_1 (SeparableCo (None, 128, 4998)    1045        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 128, 2499)    0           separable_conv1d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 2497)    49280       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 128, 1248)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 1246)    49280       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 1246)         0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)          (None, 256)          4038144     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1502)         0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1502)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1502)         6008        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1503        batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,145,260\n",
      "Trainable params: 4,142,256\n",
      "Non-trainable params: 3,004\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gs1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
